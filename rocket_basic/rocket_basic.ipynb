{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T12:39:13.968723Z",
     "start_time": "2018-02-14T12:39:12.736533Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import vizdoom as vd\n",
    "\n",
    "from tqdm import trange\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T12:39:17.172817Z",
     "start_time": "2018-02-14T12:39:17.035684Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shoot' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-53d3f17458a4>\"\u001b[1;36m, line \u001b[1;32m18\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    actions = [left, right, shoot]\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'shoot' is not defined\n"
     ]
    }
   ],
   "source": [
    "#Specify the game scenario and the screen format/resolution\n",
    "\n",
    "game = vd.DoomGame()\n",
    "game.set_screen_format(vd.ScreenFormat.BGR24)\n",
    "game.set_screen_resolution(vd.ScreenResolution.RES_640X480)\n",
    "game.load_config('rocket_basic.cfg')\n",
    "\n",
    "down_sample_ratio = 0.125\n",
    "width = int(game.get_screen_width()*down_sample_ratio)\n",
    "height = int(game.get_screen_height()*down_sample_ratio)\n",
    "channels = game.get_screen_channels()\n",
    "\n",
    "#Specify the available actions in the scenario\n",
    "\n",
    "left = [1, 0, 0]\n",
    "right = [0, 1, 0]\n",
    "#shoot = [0, 0, 1]\n",
    "actions = [left, right, shoot]\n",
    "num_actions = len(actions)\n",
    "\n",
    "#Specify the Q-network learning parameters\n",
    "\n",
    "frame_delay = 12\n",
    "buffer_size = 10000\n",
    "epochs = 30\n",
    "steps_per_epoch = 2000\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.001\n",
    "start_epsilon = 1.0\n",
    "end_epsilon = 0.1\n",
    "batch_size = 100\n",
    "load_model = True\n",
    "save_model = True\n",
    "model_dir = './checkpoints/rocket_basic.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T02:09:56.355080Z",
     "start_time": "2018-01-16T02:09:56.301528Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a buffer object that holds a set of training experiences (state-action-reward tuples)\n",
    "\n",
    "class Buffer():\n",
    "    def __init__(self, size=1000):\n",
    "        self.buffer = list()\n",
    "        self.length = len(self.buffer)\n",
    "        self.size = size\n",
    "        \n",
    "#Add new experiences to the buffer (remove old experiences if necessary to avoid exceeding the buffer size)\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        if self.length + 1 >= self.size:\n",
    "            self.buffer[0:(self.length + 1) - self.size] = []\n",
    "        \n",
    "        self.buffer.append(experience)\n",
    "        self.length = len(self.buffer)\n",
    "            \n",
    "#Return a batch of experience arrays randomly sampled from the buffer\n",
    "            \n",
    "    def sample_buffer(self, sample_size):\n",
    "        sample = np.random.randint(self.length, size=sample_size)\n",
    "        s1 = np.concatenate([self.buffer[idx][0] for idx in sample], axis=0)\n",
    "        a = np.array([self.buffer[idx][1] for idx in sample])\n",
    "        r = np.array([self.buffer[idx][2] for idx in sample])\n",
    "        s2 = np.concatenate([self.buffer[idx][3] for idx in sample], axis=0)\n",
    "        terminal = np.array([self.buffer[idx][4] for idx in sample], dtype=np.int32)\n",
    "        \n",
    "        return s1, a, r, s2, terminal\n",
    "\n",
    "#Downsample and normalize an image array representing the game state at a given time stamp\n",
    "\n",
    "def preprocess(image, down_sample_ratio=1):\n",
    "    if down_sample_ratio != 1:\n",
    "        image = scipy.misc.imresize(image, down_sample_ratio)\n",
    "    image = image.astype(np.float32)\n",
    "    image /= 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T01:46:06.891742Z",
     "start_time": "2018-01-16T01:46:06.530384Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a Q-network to estimate values and choose actions for a given state\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "s_t = tf.placeholder(tf.float32, shape=[None, height, width, channels], name='state')\n",
    "a_t = tf.placeholder(tf.int32, shape=[None], name='action')\n",
    "Q_target = tf.placeholder(tf.float32, shape=[None, num_actions], name='Q_target')\n",
    "\n",
    "input_layer = tf.reshape(s_t, [-1, height, width, channels], name='input_layer')\n",
    "conv1 = tf.layers.conv2d(inputs=input_layer,\n",
    "                         filters=32,\n",
    "                         kernel_size=[8, 8],\n",
    "                         strides=[4, 4],\n",
    "                         padding='valid',\n",
    "                         activation=tf.nn.relu,\n",
    "                         name='conv1_layer')\n",
    "conv2 = tf.layers.conv2d(inputs=conv1,\n",
    "                         filters=64,\n",
    "                         kernel_size=[4, 4],\n",
    "                         strides=[2, 2],\n",
    "                         padding='valid',\n",
    "                         activation=tf.nn.relu,\n",
    "                         name='conv2_layer')\n",
    "flatten = tf.reshape(conv2, [-1, 6*8*64], name='flatten')\n",
    "dense1 = tf.layers.dense(inputs=flatten,\n",
    "                         units=512,\n",
    "                         activation=tf.nn.relu,\n",
    "                         name='dense1_layer')\n",
    "Q_values = tf.layers.dense(inputs=dense1,\n",
    "                           units=len(actions),\n",
    "                           activation=None,\n",
    "                           name='output_layer')        \n",
    "    \n",
    "best_action = tf.argmax(Q_values, 1)\n",
    "loss = tf.losses.mean_squared_error(Q_values, Q_target)\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate, name='adam').minimize(loss)\n",
    "\n",
    "def calculate_loss(session, s, q):\n",
    "    L, _ = session.run([loss, adam], feed_dict={s_t: s, Q_target: q})\n",
    "    \n",
    "    return L\n",
    "\n",
    "#Return the array of Q-values and the best action associated with a given state\n",
    "\n",
    "def get_Q_values(session, s):\n",
    "    Q = session.run(Q_values, feed_dict={s_t: s})\n",
    "\n",
    "    return Q\n",
    "    \n",
    "def choose_action(session, s):\n",
    "    a = session.run(best_action, feed_dict={s_t: s})\n",
    "    \n",
    "    return a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T01:47:47.110154Z",
     "start_time": "2018-01-16T01:46:10.824666Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Episode 1 Reward: 73.0\n",
      "Random Episode 2 Reward: -91.0\n",
      "Random Episode 3 Reward: 67.0\n",
      "Random Episode 4 Reward: 75.0\n",
      "Random Episode 5 Reward: 75.0\n",
      "Random Episode 6 Reward: -370.0\n",
      "Random Episode 7 Reward: -33.0\n",
      "Random Episode 8 Reward: 72.0\n",
      "Random Episode 9 Reward: 18.0\n",
      "Random Episode 10 Reward: 73.0\n",
      "Random Episode 11 Reward: 68.0\n",
      "Random Episode 12 Reward: 69.0\n",
      "Random Episode 13 Reward: 73.0\n",
      "Random Episode 14 Reward: -370.0\n",
      "Random Episode 15 Reward: -117.0\n",
      "Random Episode 16 Reward: -365.0\n",
      "Random Episode 17 Reward: -370.0\n",
      "Random Episode 18 Reward: 49.0\n",
      "Random Episode 19 Reward: 75.0\n",
      "Random Episode 20 Reward: -370.0\n",
      "Average Random Reward: -64.95\n"
     ]
    }
   ],
   "source": [
    "#Play the game by choosing random actions drawn from a uniform distribution to act as a baseline example\n",
    "\n",
    "game.set_sound_enabled(True)\n",
    "game.init()\n",
    "episode_rewards = list()\n",
    "\n",
    "for i in range(20):\n",
    "    game.new_episode()\n",
    "    \n",
    "    while not game.is_episode_finished():\n",
    "        action  = np.random.randint(num_actions)\n",
    "        reward = game.make_action(actions[action])\n",
    "        \n",
    "#Insert a 0.02 second delay after each time step so that the episode is played at normal speed\n",
    "        \n",
    "        time.sleep(0.02)\n",
    "    \n",
    "    episode_rewards.append(game.get_total_reward())\n",
    "    print('Random Episode {} Reward: {}'.format(i + 1, game.get_total_reward()))\n",
    "    time.sleep(1)\n",
    "    \n",
    "game.close()\n",
    "print('Average Random Reward:', np.mean(episode_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T01:56:54.805281Z",
     "start_time": "2018-01-16T01:56:54.796772Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://drive.google.com/file/d/13MuEOfy6uY6LxSLg4L0Xsi2hjx5qnsKt/preview\" width=\"640\" height=\"480\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embed a recording of the untrained agent playing 20 episodes\n",
    "\n",
    "HTML('<iframe src=\"https://drive.google.com/file/d/13MuEOfy6uY6LxSLg4L0Xsi2hjx5qnsKt/preview\" width=\"640\" height=\"480\"></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T01:26:49.063596Z",
     "start_time": "2018-01-15T23:08:49.619317Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [08:46<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Mean Reward: -197.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [08:41<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Mean Reward: -195.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [08:15<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Mean Reward: -199.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [08:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Mean Reward: -201.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [07:03<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Mean Reward: -197.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:32<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Mean Reward: -199.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:58<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Mean Reward: -174.6505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:42<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Mean Reward: -152.6145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:12<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Mean Reward: -133.3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:27<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Mean Reward: -110.101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:11<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Mean Reward: -84.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:40<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Mean Reward: -64.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:27<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Mean Reward: -46.8185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:25<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Mean Reward: -37.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:11<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Mean Reward: -22.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:48<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Mean Reward: -10.9555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:34<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Mean Reward: 1.1865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:23<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Mean Reward: 9.6775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:24<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Mean Reward: 16.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:12<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Mean Reward: 23.3525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:02<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Mean Reward: 29.241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:53<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Mean Reward: 33.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:00<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Mean Reward: 36.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:42<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Mean Reward: 41.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:36<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Mean Reward: 44.6735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:33<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Mean Reward: 46.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:34<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Mean Reward: 49.404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:27<00:00, 13.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Mean Reward: 49.6965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:35<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Mean Reward: 50.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:30<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Mean Reward: 49.849\n",
      "639697 time steps experienced during training\n",
      "Model saved to ./checkpoints/basic.ckpt\n"
     ]
    }
   ],
   "source": [
    "#For each time step, collect the following data:\n",
    "#The current game state\n",
    "#The action that was taken taken\n",
    "#The reward obtained from the chosen action\n",
    "#The next game state (store the first game state if the previous action ends the episode)\n",
    "#A variable indicating whether the episode is over yet\n",
    "\n",
    "\n",
    "exp_buffer = Buffer(size=buffer_size)\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "game.set_sound_enabled(False)\n",
    "game.init()\n",
    "t = 0\n",
    "\n",
    "#Accumulate experiences in the buffer using an epsilon-greedy strategy with three training phases\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_rewards = list()\n",
    "    \n",
    "    for step in trange(steps_per_epoch, leave=True):\n",
    "        experience = list()\n",
    "        game.new_episode()\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            state1 = preprocess(state.screen_buffer, down_sample_ratio)\n",
    "            \n",
    "#Explore the environment by choosing random actions with 100% probability for the first phase of training\n",
    "\n",
    "            if epoch < 0.2*epochs:\n",
    "                action = np.random.randint(num_actions)\n",
    "            \n",
    "#Increase the probability of greedily choosing an action by a constant amount at each epoch in the second phase\n",
    "            \n",
    "            elif epoch < 0.9*epochs:\n",
    "                epsilon = start_epsilon - (epoch + 1 - 0.2*epochs)*(start_epsilon-end_epsilon)/(0.7*epochs)\n",
    "            \n",
    "                if np.random.uniform(0, 1) <= epsilon:\n",
    "                    action = np.random.randint(num_actions)\n",
    "                \n",
    "                else:\n",
    "                    action = choose_action(session, state1)[0]\n",
    "\n",
    "#Select a random action with 10% probability in the final phase of training\n",
    "                \n",
    "            else:\n",
    "                if np.random.uniform(0, 1) <= end_epsilon:\n",
    "                    action = np.random.randint(num_actions)\n",
    "                    \n",
    "                else:\n",
    "                    action = choose_action(session, state1)[0]\n",
    "\n",
    "            reward = game.make_action(actions[action], frame_delay)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done == False:\n",
    "                state = game.get_state()\n",
    "                state2 = preprocess(state.screen_buffer, down_sample_ratio)\n",
    "        \n",
    "            elif done == True:\n",
    "                state2 = state1\n",
    "        \n",
    "#Add the experience obtained from each time step to the buffer\n",
    "\n",
    "            t += 1\n",
    "            exp_buffer.add_experience((state1, action, reward, state2, done))\n",
    "        \n",
    "#Sample a minibatch from the buffer if there are enough experiences in the buffer\n",
    "\n",
    "        if exp_buffer.length > batch_size:\n",
    "            s1, a, r, s2, terminal = exp_buffer.sample_buffer(batch_size)\n",
    "            \n",
    "#Train the Q-network by using the minibatch to update the action-value function Q\n",
    "            \n",
    "            Q2 = np.max(get_Q_values(session, s2), axis=1)\n",
    "            target_Q = get_Q_values(session, s1)\n",
    "            target_Q[np.arange(batch_size), a] = r + discount_factor*(1 - terminal)*Q2\n",
    "            calculate_loss(session, s1, target_Q)\n",
    "            \n",
    "        epoch_rewards.append(game.get_total_reward())\n",
    "        \n",
    "    print('Epoch {} Mean Reward: {}'.format(epoch + 1, np.mean(epoch_rewards)))\n",
    "        \n",
    "print('{} time steps experienced during training'.format(t))\n",
    "game.close()\n",
    "\n",
    "if save_model == True:\n",
    "    print('Model saved to', model_dir)\n",
    "    tf.train.Saver().save(session, model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T01:33:16.119443Z",
     "start_time": "2018-01-16T01:32:26.418507Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./checkpoints/rocket_basic.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rocket_basic.ckpt\n",
      "Test Episode 1 Reward: 71.0\n",
      "Test Episode 2 Reward: 71.0\n",
      "Test Episode 3 Reward: 71.0\n",
      "Test Episode 4 Reward: 71.0\n",
      "Test Episode 5 Reward: 69.0\n",
      "Test Episode 6 Reward: 71.0\n",
      "Test Episode 7 Reward: 71.0\n",
      "Test Episode 8 Reward: 71.0\n",
      "Test Episode 9 Reward: 71.0\n",
      "Test Episode 10 Reward: 71.0\n",
      "Test Episode 11 Reward: 71.0\n",
      "Test Episode 12 Reward: 71.0\n",
      "Test Episode 13 Reward: 56.0\n",
      "Test Episode 14 Reward: 71.0\n",
      "Test Episode 15 Reward: 71.0\n",
      "Test Episode 16 Reward: 71.0\n",
      "Test Episode 17 Reward: 71.0\n",
      "Test Episode 18 Reward: 71.0\n",
      "Test Episode 19 Reward: 30.0\n",
      "Test Episode 20 Reward: 71.0\n",
      "Average Test Reward: 68.1\n"
     ]
    }
   ],
   "source": [
    "#Test the fully trained model by only choosing actions with a greedy strategy\n",
    "\n",
    "if load_model == True:\n",
    "    session = tf.Session()\n",
    "    print('Loading model from', model_dir)\n",
    "    tf.train.Saver().restore(session, model_dir)\n",
    "\n",
    "game.set_sound_enabled(True)\n",
    "game.init()\n",
    "episode_rewards = list()\n",
    "\n",
    "for i in range(20):\n",
    "    game.new_episode()\n",
    "    \n",
    "    while not game.is_episode_finished():\n",
    "        state = game.get_state()\n",
    "        state1 = preprocess(state.screen_buffer, down_sample_ratio)\n",
    "        action = choose_action(session, state1)[0]\n",
    "        reward = game.make_action(actions[action])\n",
    "        time.sleep(0.02)\n",
    "        \n",
    "    episode_rewards.append(game.get_total_reward())\n",
    "    print('Test Episode {} Reward: {}'.format(i + 1, game.get_total_reward()))\n",
    "    time.sleep(1)\n",
    "    \n",
    "game.close()\n",
    "print('Average Test Reward:', np.mean(episode_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T01:53:53.982733Z",
     "start_time": "2018-01-16T01:53:53.972723Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://drive.google.com/file/d/1nbnPwtomrITlh-QdSXCPA6S85RlOlVzW/preview\" width=\"640\" height=\"480\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embed a recording of the fully trained agent playing 20 episodes\n",
    "\n",
    "HTML('<iframe src=\"https://drive.google.com/file/d/1nbnPwtomrITlh-QdSXCPA6S85RlOlVzW/preview\" width=\"640\" height=\"480\"></iframe>')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
