{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T00:39:26.289965Z",
     "start_time": "2018-01-23T00:39:25.004724Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import scipy.misc\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import trange\n",
    "from IPython.display import HTML\n",
    "\n",
    "#Import the vizdoom package as \"vd\" since it can't be installed normally on Windows\n",
    "\n",
    "vd_location = 'C:/Anaconda3/envs/doom/Lib/site-packages/vizdoom/vizdoom.pyd'\n",
    "vizdoom = importlib.util.spec_from_file_location('vizdoom',\n",
    "                                                 vd_location)\n",
    "vd = importlib.util.module_from_spec(vizdoom)\n",
    "vizdoom.loader.exec_module(vd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T00:39:26.440110Z",
     "start_time": "2018-01-23T00:39:26.290966Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Specify the game scenario and the screen format/resolution\n",
    "\n",
    "game = vd.DoomGame()\n",
    "game.set_screen_format(vd.ScreenFormat.BGR24)\n",
    "game.set_depth_buffer_enabled(True)\n",
    "game.set_screen_resolution(vd.ScreenResolution.RES_160X120)\n",
    "game.load_config('deadly_corridor.cfg')\n",
    "\n",
    "down_sample_ratio = 0.5\n",
    "width = int(game.get_screen_width()*down_sample_ratio)\n",
    "height = int(game.get_screen_height()*down_sample_ratio)\n",
    "channels = game.get_screen_channels() + 1\n",
    "\n",
    "#Specify the available actions in the scenario\n",
    "\n",
    "available_actions = game.get_available_buttons()\n",
    "actions = [list(ohe) for ohe in list(np.identity(len(available_actions)))]\n",
    "num_actions = len(available_actions)\n",
    "\n",
    "#Specify the Q-network learning parameters\n",
    "\n",
    "frame_delay = 12\n",
    "buffer_size = 50000\n",
    "epochs = 80\n",
    "steps_per_epoch = 2000\n",
    "discount_factor = 0.75\n",
    "learning_rate = 0.001\n",
    "start_epsilon = 1.0\n",
    "end_epsilon = 0.1\n",
    "batch_size = 100\n",
    "load_model = False\n",
    "save_model = True\n",
    "model_dir = './checkpoints/deadly_corridor.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T00:39:26.759419Z",
     "start_time": "2018-01-23T00:39:26.441612Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a buffer object that holds a set of training experiences (state-action-reward tuples)\n",
    "\n",
    "class Buffer():\n",
    "    def __init__(self, size=1000):\n",
    "        self.buffer = list()\n",
    "        self.length = len(self.buffer)\n",
    "        self.size = size\n",
    "        \n",
    "#Add a new experience to the buffer (remove the oldest experience if the buffer is already full)\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        if self.length + 1 >= self.size:\n",
    "            self.buffer[0:(self.length + 1) - self.size] = []\n",
    "        \n",
    "        self.buffer.append(experience)\n",
    "        self.length = len(self.buffer)\n",
    "            \n",
    "#Return a batch of experience arrays randomly sampled from the buffer\n",
    "            \n",
    "    def sample_buffer(self, sample_size):\n",
    "        sample = np.random.randint(self.length, size=sample_size)\n",
    "        s1 = np.concatenate([self.buffer[idx][0] for idx in sample], axis=0)\n",
    "        a = np.array([self.buffer[idx][1] for idx in sample])\n",
    "        r = np.array([self.buffer[idx][2] for idx in sample])\n",
    "        s2 = np.concatenate([self.buffer[idx][3] for idx in sample], axis=0)\n",
    "        terminal = np.array([self.buffer[idx][4] for idx in sample], dtype=np.int32)\n",
    "        \n",
    "        return s1, a, r, s2, terminal\n",
    "\n",
    "#Downsample and normalize an image array representing the game state at a given time stamp\n",
    "\n",
    "def preprocess(image, down_sample_ratio=1):\n",
    "    if down_sample_ratio != 1:\n",
    "        image = scipy.misc.imresize(image, down_sample_ratio)\n",
    "    image = image.astype(np.float32)\n",
    "    image /= 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "#Test the agent using a currently training or previously trained model\n",
    "\n",
    "def test_agent(model, num_episodes, load_model, training=True, session=None, model_dir=None):\n",
    "    if load_model == True:\n",
    "        sess = tf.Session()\n",
    "        print('Loading model from', model_dir)\n",
    "        tf.train.Saver().restore(sess, model_dir)\n",
    "        \n",
    "#Require an existing session if a pretrained model isn't provided\n",
    "        \n",
    "    elif load_model == False:\n",
    "        sess = session\n",
    "\n",
    "    game.set_sound_enabled(True)\n",
    "    episode_rewards = list()\n",
    "    \n",
    "#Avoid reinitializing the game if this was already done by the training process\n",
    "    \n",
    "    if training == False:\n",
    "        game.init()\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        game.new_episode()\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            buffer = np.concatenate((state.screen_buffer,\n",
    "                                     np.expand_dims(state.depth_buffer,\n",
    "                                                    axis=2)),\n",
    "                                    axis=2)\n",
    "            state1 = preprocess(buffer, down_sample_ratio)\n",
    "            action = model.choose_action(sess, state1)[0]\n",
    "            reward = game.make_action(actions[action])\n",
    "            \n",
    "#Add a delay between each time step so that the episodes occur at normal speed\n",
    "\n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        episode_rewards.append(game.get_total_reward())\n",
    "        print('Test Episode {} Reward: {}'.format(i + 1, game.get_total_reward()))\n",
    "        time.sleep(1)\n",
    "    \n",
    "#Avoid ending the game so that the training process can continue\n",
    "    \n",
    "    if training == False:\n",
    "        game.close()\n",
    "    \n",
    "    return ('Average Test Reward:', np.mean(episode_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T00:39:27.043193Z",
     "start_time": "2018-01-23T00:39:26.760420Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a Q-network to estimate values and choose actions for a given state\n",
    "\n",
    "class Q_network():\n",
    "    def __init__(self, network_name, height, width, channels, learning_rate=0.001):\n",
    "        self.s_t = tf.placeholder(tf.float32,\n",
    "                                  shape=[None, height, width, channels],\n",
    "                                  name=network_name + '_state'\n",
    "                                 )\n",
    "        self.a_t = tf.placeholder(tf.int32,\n",
    "                                  shape=[None],\n",
    "                                  name=network_name + '_action'\n",
    "                                 )\n",
    "        self.Q_target = tf.placeholder(tf.float32,\n",
    "                                       shape=[None, num_actions],\n",
    "                                       name=network_name + '_Q_target'\n",
    "                                      )\n",
    "\n",
    "        self.input_layer = tf.reshape(self.s_t,\n",
    "                                      [-1, height, width, channels],\n",
    "                                      name=network_name + '_input_layer'\n",
    "                                     )\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input_layer,\n",
    "                                      filters=32,\n",
    "                                      kernel_size=[8, 8],\n",
    "                                      strides=[4, 4],\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name=network_name + '_conv1_layer'\n",
    "                                     )\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
    "                                      filters=64,\n",
    "                                      kernel_size=[4, 4],\n",
    "                                      strides=[2, 2],\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name=network_name + '_conv2_layer'\n",
    "                                     )\n",
    "        self.flatten = tf.reshape(self.conv2,\n",
    "                                  [-1, 6*8*64],\n",
    "                                  name=network_name + '_flatten'\n",
    "                                 )\n",
    "        self.dense = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=512,\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name=network_name + '_dense1_layer'\n",
    "                                    )\n",
    "        self.Q_values = tf.layers.dense(inputs=self.dense,\n",
    "                                        units=len(actions),\n",
    "                                        activation=None,\n",
    "                                        name=network_name + '_output_layer'\n",
    "                                       )        \n",
    "    \n",
    "        self.best_action = tf.argmax(self.Q_values, 1)\n",
    "        self.loss = tf.losses.mean_squared_error(self.Q_values,\n",
    "                                                 self.Q_target)\n",
    "        self.adam = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                           name=network_name + '_adam'\n",
    "                                          )\n",
    "        self.train = self.adam.minimize(self.loss)\n",
    "\n",
    "    def calculate_loss(self, session, s, q):\n",
    "        L, _ = session.run([self.loss, self.train],\n",
    "                           feed_dict={self.s_t: s,\n",
    "                                      self.Q_target: q})\n",
    "    \n",
    "        return L\n",
    "\n",
    "#Return the array of Q-values and the best action associated with a given state\n",
    "\n",
    "    def get_Q_values(self, session, s):\n",
    "        Q = session.run(self.Q_values,\n",
    "                        feed_dict={self.s_t: s})\n",
    "\n",
    "        return Q\n",
    "    \n",
    "    def choose_action(self, session, s):\n",
    "        a = session.run(self.best_action,\n",
    "                        feed_dict={self.s_t: s})\n",
    "    \n",
    "        return a\n",
    "    \n",
    "#Create a list of variable update operations\n",
    "\n",
    "def update_graph(variables):\n",
    "    update_ops = list()\n",
    "    \n",
    "#Assign weight values from the network created first to the one created second\n",
    "    \n",
    "    for idx, variable in enumerate(variables[:len(variables)//2]):\n",
    "        op = variable.assign(variables[idx + len(variables)//2].value())\n",
    "        update_ops.append(op)\n",
    "    \n",
    "    return update_ops\n",
    "\n",
    "#Update the target network parameters to match those of the online network\n",
    "\n",
    "def update_target(ops, session):\n",
    "    for op in update_ops:\n",
    "        session.run(op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:47:29.463735Z",
     "start_time": "2018-01-23T00:39:27.044194Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:37<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Mean Reward: 130.22627645111083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:52<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Mean Reward: 134.51816317749024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:11<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Mean Reward: 130.79105252075195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:06<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Mean Reward: 130.91410857391358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:58<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Mean Reward: 130.57677927398683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:24<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Mean Reward: 132.42309548950195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:56<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Mean Reward: 131.9567905883789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:56<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Mean Reward: 131.45893687438965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:48<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Mean Reward: 132.1183303833008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:46<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Mean Reward: 133.49377787780762\n",
      "Epoch 10 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 10 test:\n",
      "Test Episode 1 Reward: 226.78720092773438\n",
      "Test Episode 2 Reward: 541.5596160888672\n",
      "Test Episode 3 Reward: 344.442138671875\n",
      "Test Episode 4 Reward: 344.442138671875\n",
      "Test Episode 5 Reward: 344.442138671875\n",
      "Test Episode 6 Reward: 344.442138671875\n",
      "Test Episode 7 Reward: 344.442138671875\n",
      "Test Episode 8 Reward: 344.442138671875\n",
      "Test Episode 9 Reward: 344.442138671875\n",
      "Test Episode 10 Reward: 344.442138671875\n",
      "('Average Test Reward:', 352.38839263916014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:13<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Mean Reward: 130.14234243011475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:16<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Mean Reward: 132.95560752105712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:52<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Mean Reward: 131.0480877685547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:53<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Mean Reward: 128.0188042755127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:55<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Mean Reward: 130.37146461486816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:48<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Mean Reward: 131.2083526763916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:47<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Mean Reward: 132.5362410812378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:50<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Mean Reward: 130.49721478271485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:49<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Mean Reward: 131.60849320983885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:50<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Mean Reward: 129.30863298797607\n",
      "Epoch 20 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 20 test:\n",
      "Test Episode 1 Reward: 279.0060729980469\n",
      "Test Episode 2 Reward: 510.0224151611328\n",
      "Test Episode 3 Reward: 279.0060729980469\n",
      "Test Episode 4 Reward: 315.7782897949219\n",
      "Test Episode 5 Reward: 279.0060729980469\n",
      "Test Episode 6 Reward: 563.895263671875\n",
      "Test Episode 7 Reward: 98.09750366210938\n",
      "Test Episode 8 Reward: 619.8581237792969\n",
      "Test Episode 9 Reward: 279.0060729980469\n",
      "Test Episode 10 Reward: 279.0060729980469\n",
      "('Average Test Reward:', 350.26819610595703)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:47<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Mean Reward: 131.9026269683838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:46<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Mean Reward: 129.11614985656738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:44<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Mean Reward: 131.10017154693602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:44<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Mean Reward: 132.2241854171753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:24<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Mean Reward: 164.55961542510985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:13<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Mean Reward: 165.9850161819458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:14<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Mean Reward: 172.69815673065185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:23<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Mean Reward: 179.77751403808594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:20<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Mean Reward: 181.11722457885742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:23<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Mean Reward: 187.4103791732788\n",
      "Epoch 30 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 30 test:\n",
      "Test Episode 1 Reward: 294.95274353027344\n",
      "Test Episode 2 Reward: 294.95274353027344\n",
      "Test Episode 3 Reward: 628.3979644775391\n",
      "Test Episode 4 Reward: 294.95274353027344\n",
      "Test Episode 5 Reward: 453.07073974609375\n",
      "Test Episode 6 Reward: 436.69554138183594\n",
      "Test Episode 7 Reward: 294.95274353027344\n",
      "Test Episode 8 Reward: 294.95274353027344\n",
      "Test Episode 9 Reward: 646.2454071044922\n",
      "Test Episode 10 Reward: 316.8426818847656\n",
      "('Average Test Reward:', 395.60160522460939)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:27<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Mean Reward: 193.75594888305665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:22<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Mean Reward: 195.87126916503905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:24<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Mean Reward: 199.13965762329101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:24<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Mean Reward: 203.63886737060548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:26<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Mean Reward: 212.4102200012207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:24<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Mean Reward: 211.66010665893555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:31<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Mean Reward: 220.39235485076904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:32<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Mean Reward: 222.24457292175293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:28<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Mean Reward: 225.80115264892578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:31<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Mean Reward: 235.4856062850952\n",
      "Epoch 40 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 40 test:\n",
      "Test Episode 1 Reward: 288.68162536621094\n",
      "Test Episode 2 Reward: 523.4003448486328\n",
      "Test Episode 3 Reward: 288.68162536621094\n",
      "Test Episode 4 Reward: 572.4085998535156\n",
      "Test Episode 5 Reward: 244.4740753173828\n",
      "Test Episode 6 Reward: 317.6009826660156\n",
      "Test Episode 7 Reward: 505.3304901123047\n",
      "Test Episode 8 Reward: 350.8273620605469\n",
      "Test Episode 9 Reward: 288.68162536621094\n",
      "Test Episode 10 Reward: 259.5741424560547\n",
      "('Average Test Reward:', 363.96608734130859)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:31<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Mean Reward: 237.11937521362304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:27<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Mean Reward: 241.99600649261475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:27<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Mean Reward: 239.86270484161378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:33<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Mean Reward: 253.58983532714845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:37<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Mean Reward: 250.91648341369628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:37<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Mean Reward: 259.4569700241089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:38<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Mean Reward: 262.80324201965334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:37<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Mean Reward: 265.3439067230225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:39<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Mean Reward: 271.24021926116944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:41<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Mean Reward: 282.3534098129272\n",
      "Epoch 50 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 50 test:\n",
      "Test Episode 1 Reward: 288.6736145019531\n",
      "Test Episode 2 Reward: 260.38990783691406\n",
      "Test Episode 3 Reward: 288.6736145019531\n",
      "Test Episode 4 Reward: 316.6221618652344\n",
      "Test Episode 5 Reward: 288.6736145019531\n",
      "Test Episode 6 Reward: 288.6736145019531\n",
      "Test Episode 7 Reward: 281.7825622558594\n",
      "Test Episode 8 Reward: 1041.0503540039062\n",
      "Test Episode 9 Reward: 327.6304473876953\n",
      "Test Episode 10 Reward: 588.8087158203125\n",
      "('Average Test Reward:', 397.09786071777341)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:42<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Mean Reward: 294.8150086288452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:42<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Mean Reward: 292.51391342163083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:40<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Mean Reward: 308.40388306427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:33<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Mean Reward: 313.07802807617185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:31<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Mean Reward: 317.4867276687622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:37<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Mean Reward: 324.30731047058106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:35<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Mean Reward: 331.31592361450197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:38<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Mean Reward: 338.96638208770753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:43<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Mean Reward: 351.65807095336913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:43<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Mean Reward: 361.01266443634034\n",
      "Epoch 60 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 60 test:\n",
      "Test Episode 1 Reward: 288.68162536621094\n",
      "Test Episode 2 Reward: 288.68162536621094\n",
      "Test Episode 3 Reward: 554.5430603027344\n",
      "Test Episode 4 Reward: 288.68162536621094\n",
      "Test Episode 5 Reward: 288.68162536621094\n",
      "Test Episode 6 Reward: 259.55804443359375\n",
      "Test Episode 7 Reward: 288.68162536621094\n",
      "Test Episode 8 Reward: 565.3264770507812\n",
      "Test Episode 9 Reward: 288.68162536621094\n",
      "Test Episode 10 Reward: 288.68162536621094\n",
      "('Average Test Reward:', 340.01989593505857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:38<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Mean Reward: 364.13893077850344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:27<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Mean Reward: 374.53940602111817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:51<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Mean Reward: 376.26286662292483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:17<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Mean Reward: 379.5990826873779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:15<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Mean Reward: 387.723549369812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:53<00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Mean Reward: 399.78635751342773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:50<00:00, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Mean Reward: 408.8314730758667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:48<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Mean Reward: 416.6059064788818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:47<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Mean Reward: 425.6775333328247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:48<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Mean Reward: 432.81911361694335\n",
      "Epoch 70 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 70 test:\n",
      "Test Episode 1 Reward: 287.1919708251953\n",
      "Test Episode 2 Reward: 287.1919708251953\n",
      "Test Episode 3 Reward: 562.4911346435547\n",
      "Test Episode 4 Reward: 287.1919708251953\n",
      "Test Episode 5 Reward: 769.7454833984375\n",
      "Test Episode 6 Reward: 676.4645385742188\n",
      "Test Episode 7 Reward: 492.7627716064453\n",
      "Test Episode 8 Reward: 351.0941467285156\n",
      "Test Episode 9 Reward: 287.1919708251953\n",
      "Test Episode 10 Reward: 257.4929504394531\n",
      "('Average Test Reward:', 425.88189086914065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:50<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Mean Reward: 440.33210068511966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:47<00:00, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Mean Reward: 443.35806465148926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:08<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Mean Reward: 447.29509090423585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:21<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Mean Reward: 446.39959513092043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:35<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Mean Reward: 444.86860160064697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:48<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Mean Reward: 448.83525344085695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:46<00:00, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Mean Reward: 447.6067451324463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:52<00:00, 17.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Mean Reward: 442.4372561645508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:49<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Mean Reward: 451.96415175628664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:50<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Mean Reward: 444.92636431121826\n",
      "Epoch 80 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 80 test:\n",
      "Test Episode 1 Reward: 288.68162536621094\n",
      "Test Episode 2 Reward: 265.7804718017578\n",
      "Test Episode 3 Reward: 288.68162536621094\n",
      "Test Episode 4 Reward: 524.2005310058594\n",
      "Test Episode 5 Reward: 549.0877685546875\n",
      "Test Episode 6 Reward: 288.68162536621094\n",
      "Test Episode 7 Reward: 517.6818084716797\n",
      "Test Episode 8 Reward: 561.9625091552734\n",
      "Test Episode 9 Reward: 288.68162536621094\n",
      "Test Episode 10 Reward: 288.68162536621094\n",
      "('Average Test Reward:', 386.21212158203127)\n",
      "[(451.96415175628664, 79), (448.83525344085695, 76), (447.60674513244629, 77), (447.29509090423585, 73), (446.39959513092043, 74), (444.92636431121826, 80), (444.86860160064697, 75), (443.35806465148926, 72), (442.43725616455077, 78), (440.33210068511966, 71), (432.81911361694335, 70), (425.67753333282468, 69), (416.60590647888182, 68), (408.83147307586671, 67), (399.78635751342773, 66), (387.72354936981202, 65), (379.59908268737792, 64), (376.26286662292483, 63), (374.53940602111817, 62), (364.13893077850344, 61), (361.01266443634034, 60), (351.65807095336913, 59), (338.96638208770753, 58), (331.31592361450197, 57), (324.30731047058106, 56), (317.48672766876223, 55), (313.07802807617185, 54), (308.40388306427002, 53), (294.81500862884519, 51), (292.51391342163083, 52), (282.35340981292723, 50), (271.24021926116944, 49), (265.34390672302249, 48), (262.80324201965334, 47), (259.4569700241089, 46), (253.58983532714845, 44), (250.91648341369628, 45), (241.99600649261475, 42), (239.86270484161378, 43), (237.11937521362304, 41), (235.4856062850952, 40), (225.80115264892578, 39), (222.24457292175293, 38), (220.39235485076904, 37), (212.41022000122069, 35), (211.66010665893555, 36), (203.63886737060548, 34), (199.13965762329101, 33), (195.87126916503905, 32), (193.75594888305665, 31), (187.41037917327881, 30), (181.11722457885742, 29), (179.77751403808594, 28), (172.69815673065185, 27), (165.98501618194581, 26), (164.55961542510985, 25), (134.51816317749024, 2), (133.49377787780762, 10), (132.95560752105712, 12), (132.53624108123779, 17), (132.42309548950195, 6), (132.22418541717531, 24), (132.11833038330079, 9), (131.95679058837891, 7), (131.9026269683838, 21), (131.60849320983885, 19), (131.45893687438965, 8), (131.2083526763916, 16), (131.10017154693602, 23), (131.04808776855469, 13), (130.91410857391358, 4), (130.79105252075195, 3), (130.57677927398683, 5), (130.49721478271485, 18), (130.37146461486816, 15), (130.22627645111083, 1), (130.14234243011475, 11), (129.30863298797607, 20), (129.11614985656738, 22), (128.01880427551271, 14)]\n",
      "1207895 time steps experienced during training\n"
     ]
    }
   ],
   "source": [
    "#For each time step, collect the following data:\n",
    "#The current game state\n",
    "#The action that was taken taken\n",
    "#The reward obtained from the chosen action\n",
    "#The next game state (store the first game state if the previous action ends the episode)\n",
    "#A variable indicating whether the episode is over yet\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Instantiate the target network before the online network so that it's updated correctly\n",
    "\n",
    "target_net = Q_network(network_name='target',\n",
    "                       learning_rate=learning_rate,\n",
    "                       height=height,\n",
    "                       width=width,\n",
    "                       channels=channels)\n",
    "DQN = Q_network(network_name='online',\n",
    "                learning_rate=learning_rate,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                channels=channels)\n",
    "\n",
    "exp_buffer = Buffer(size=buffer_size)\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "weights = tf.trainable_variables()\n",
    "\n",
    "update_ops = update_graph(weights)\n",
    "\n",
    "if load_model == True:\n",
    "    print('Loading model from', model_dir)\n",
    "    tf.train.Saver().restore(session, model_dir)\n",
    "    \n",
    "elif load_model == False:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "game.set_sound_enabled(False)\n",
    "game.init()\n",
    "\n",
    "t = 0\n",
    "epoch_rank = list()\n",
    "\n",
    "#Accumulate experiences in the buffer using an epsilon-greedy strategy with three training phases\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_rewards = list()\n",
    "    \n",
    "    for step in trange(steps_per_epoch, leave=True):\n",
    "        experience = list()\n",
    "        game.new_episode()\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            state1 = preprocess(np.concatenate((state.screen_buffer,\n",
    "                                                np.expand_dims(state.depth_buffer, axis=2)),\n",
    "                                                axis=2),\n",
    "                                               down_sample_ratio)\n",
    "            \n",
    "#Explore the environment by choosing random actions with 100% probability for the first phase of training\n",
    "\n",
    "            if epoch < 0.3*epochs:\n",
    "                action = np.random.randint(num_actions)\n",
    "            \n",
    "#Increase the probability of greedily choosing an action by a constant amount at each epoch in the second phase\n",
    "            \n",
    "            elif epoch < 0.9*epochs:\n",
    "                epsilon = start_epsilon - (epoch + 1 - 0.2*epochs)*(start_epsilon-end_epsilon)/(0.7*epochs)\n",
    "            \n",
    "                if np.random.uniform(0, 1) <= epsilon:\n",
    "                    action = np.random.randint(num_actions)\n",
    "                \n",
    "                else:\n",
    "                    action = DQN.choose_action(session, state1)[0]\n",
    "\n",
    "#Select a random action with 10% probability in the final phase of training\n",
    "                \n",
    "            else:\n",
    "                if np.random.uniform(0, 1) <= end_epsilon:\n",
    "                    action = np.random.randint(num_actions)\n",
    "                    \n",
    "                else:\n",
    "                    action = DQN.choose_action(session, state1)[0]\n",
    "\n",
    "            reward = game.make_action(actions[action], frame_delay)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done == False:\n",
    "                state = game.get_state()\n",
    "                state2 = preprocess(np.concatenate((state.screen_buffer,\n",
    "                                                    np.expand_dims(state.depth_buffer, axis=2)),\n",
    "                                                    axis=2),\n",
    "                                                    down_sample_ratio)\n",
    "        \n",
    "            elif done == True:\n",
    "                state2 = state1\n",
    "        \n",
    "#Add the experience obtained from each time step to the buffer\n",
    "\n",
    "            t += 1\n",
    "            exp_buffer.add_experience((state1, action, reward, state2, done))\n",
    "        \n",
    "#Sample a minibatch from the buffer if there are enough experiences in the buffer\n",
    "\n",
    "        if exp_buffer.length > batch_size:\n",
    "            s1, a, r, s2, terminal = exp_buffer.sample_buffer(batch_size)\n",
    "            \n",
    "#Get the target values from the target Q-network\n",
    "            \n",
    "            target_Q = np.max(target_net.get_Q_values(session, s2), axis=1)\n",
    "            \n",
    "#Train the online Q-network by using a minibatch to update the action-value function\n",
    "\n",
    "            Q2 = DQN.get_Q_values(session, s1)\n",
    "            Q2[np.arange(batch_size), a] = r + discount_factor*(1 - terminal)*target_Q\n",
    "            DQN.calculate_loss(session, s1, Q2)\n",
    "            \n",
    "        epoch_rewards.append(game.get_total_reward())\n",
    "    \n",
    "    print('Epoch {} Mean Reward: {}'.format(epoch + 1, np.mean(epoch_rewards)))\n",
    "    \n",
    "#Save the model, update the target network, and test the agent for 10 episodes every 10 epochs\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 and epoch > 0:\n",
    "        epoch_rank.append((np.mean(epoch_rewards), epoch + 1))\n",
    "        \n",
    "        if save_model == True:\n",
    "            checkpoint = model_dir + '-' + str(epoch + 1)\n",
    "            print('Epoch {} Model saved to {}'.format(epoch + 1, model_dir))\n",
    "            saver.save(session, model_dir, global_step=epoch + 1)\n",
    "            \n",
    "        update_target(update_ops, session)\n",
    "\n",
    "        print('Epoch {} test:'.format(epoch + 1))\n",
    "        print(test_agent(DQN, num_episodes=10,\n",
    "                         training=True,\n",
    "                         load_model=False,\n",
    "                         session=session,\n",
    "                         model_dir=model_dir))\n",
    "        \n",
    "#Return a sorted list of epoch checkpoints based on average test episode reward\n",
    "        \n",
    "print(sorted(epoch_rank, reverse=True))\n",
    "print('{} time steps experienced during training'.format(t))\n",
    "game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:48:19.559607Z",
     "start_time": "2018-01-23T04:47:29.464735Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints\\deadly_corridor.ckpt-50\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\deadly_corridor.ckpt-50\n",
      "Test Episode 1 Reward: 288.6736145019531\n",
      "Test Episode 2 Reward: 318.01182556152344\n",
      "Test Episode 3 Reward: 478.78614807128906\n",
      "Test Episode 4 Reward: 288.6736145019531\n",
      "Test Episode 5 Reward: 264.34947204589844\n",
      "Test Episode 6 Reward: 652.1700592041016\n",
      "Test Episode 7 Reward: 221.61668395996094\n",
      "Test Episode 8 Reward: 288.6736145019531\n",
      "Test Episode 9 Reward: 288.6736145019531\n",
      "Test Episode 10 Reward: 511.9719543457031\n",
      "Test Episode 11 Reward: 288.6736145019531\n",
      "Test Episode 12 Reward: 309.7176055908203\n",
      "Test Episode 13 Reward: 275.09674072265625\n",
      "Test Episode 14 Reward: 524.8068237304688\n",
      "Test Episode 15 Reward: 324.20448303222656\n",
      "Test Episode 16 Reward: 288.6736145019531\n",
      "Test Episode 17 Reward: 288.6736145019531\n",
      "Test Episode 18 Reward: 520.2238616943359\n",
      "Test Episode 19 Reward: 304.08660888671875\n",
      "Test Episode 20 Reward: 288.6736145019531\n",
      "('Average Test Reward:', 350.72155914306643)\n"
     ]
    }
   ],
   "source": [
    "#Get a list of checkpoints saved during training\n",
    "\n",
    "ckpts = tf.train.get_checkpoint_state('checkpoints').all_model_checkpoint_paths\n",
    "\n",
    "#Test the trained model from a certain checkpoint by only choosing actions with a greedy strategy\n",
    "\n",
    "print(test_agent(DQN, num_episodes=20, training=False, load_model=True, model_dir=ckpts[-4]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
