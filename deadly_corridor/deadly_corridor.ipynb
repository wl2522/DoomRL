{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-27T15:43:14.239429Z",
     "start_time": "2018-01-27T15:43:12.763995Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from skimage.transform import rescale\n",
    "from tqdm import trange\n",
    "from IPython.display import HTML\n",
    "\n",
    "#Import the vizdoom package as \"vd\" since it can't be installed normally on Windows\n",
    "\n",
    "vd_location = 'C:/Anaconda3/envs/doom/Lib/site-packages/vizdoom/vizdoom.pyd'\n",
    "vizdoom = importlib.util.spec_from_file_location('vizdoom',\n",
    "                                                 vd_location)\n",
    "vd = importlib.util.module_from_spec(vizdoom)\n",
    "vizdoom.loader.exec_module(vd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-27T15:43:14.388073Z",
     "start_time": "2018-01-27T15:43:14.240431Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Specify the game scenario and the screen format/resolution\n",
    "\n",
    "game = vd.DoomGame()\n",
    "game.set_screen_format(vd.ScreenFormat.BGR24)\n",
    "game.set_depth_buffer_enabled(True)\n",
    "game.set_screen_resolution(vd.ScreenResolution.RES_160X120)\n",
    "game.load_config('deadly_corridor.cfg')\n",
    "\n",
    "down_sample_ratio = 0.5\n",
    "width = int(game.get_screen_width()*down_sample_ratio)\n",
    "height = int(game.get_screen_height()*down_sample_ratio)\n",
    "channels = game.get_screen_channels() + int(game.is_depth_buffer_enabled())\n",
    "\n",
    "#Specify the available actions in the scenario\n",
    "\n",
    "available_actions = game.get_available_buttons()\n",
    "actions = [list(ohe) for ohe in list(np.identity(len(available_actions)))]\n",
    "num_actions = len(available_actions)\n",
    "\n",
    "#Specify the Q-network learning parameters\n",
    "\n",
    "frame_delay = 12\n",
    "buffer_size = 100000\n",
    "epochs = 500\n",
    "steps_per_epoch = 2000\n",
    "learning_rate = 0.005\n",
    "gamma = 0\n",
    "start_epsilon = 1.0\n",
    "end_epsilon = 0.1\n",
    "batch_size = 100\n",
    "load_model = False\n",
    "save_model = True\n",
    "model_dir = './checkpoints/deadly_corridor.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-27T15:43:14.736910Z",
     "start_time": "2018-01-27T15:43:14.389074Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a buffer object that holds a set of training experiences (state-action-reward tuples)\n",
    "\n",
    "class Buffer():\n",
    "    def __init__(self, size=1000):\n",
    "        self.buffer = list()\n",
    "        self.length = len(self.buffer)\n",
    "        self.size = size\n",
    "        \n",
    "#Add a new experience to the buffer (remove the oldest experience if the buffer is already full)\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        if self.length + 1 >= self.size:\n",
    "            self.buffer[0:(self.length + 1) - self.size] = []\n",
    "        \n",
    "        self.buffer.append(experience)\n",
    "        self.length = len(self.buffer)\n",
    "            \n",
    "#Return a batch of experience arrays randomly sampled from the buffer\n",
    "            \n",
    "    def sample_buffer(self, sample_size):\n",
    "        sample = np.random.randint(self.length, size=sample_size)\n",
    "        s1 = np.concatenate([self.buffer[idx][0] for idx in sample], axis=0)\n",
    "        a = np.array([self.buffer[idx][1] for idx in sample])\n",
    "        r = np.array([self.buffer[idx][2] for idx in sample])\n",
    "        s2 = np.concatenate([self.buffer[idx][3] for idx in sample], axis=0)\n",
    "        terminal = np.array([self.buffer[idx][4] for idx in sample], dtype=np.int32)\n",
    "        \n",
    "        return s1, a, r, s2, terminal\n",
    "\n",
    "#Downsample and normalize an image array representing the game state at a given time stamp\n",
    "\n",
    "def preprocess(image, down_sample_ratio=1):\n",
    "    if down_sample_ratio != 1:\n",
    "        image = rescale(image=image, scale=down_sample_ratio, mode='reflect')\n",
    "    image = image.astype(np.float32)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    return image\n",
    "\n",
    "#Test the agent using a currently training or previously trained model\n",
    "\n",
    "def test_agent(model, num_episodes, load_model, depth, training=True, session=None, model_dir=None):\n",
    "    if load_model == True:\n",
    "        sess = tf.Session()\n",
    "        print('Loading model from', model_dir)\n",
    "        tf.train.Saver().restore(sess, model_dir)\n",
    "        \n",
    "#Require an existing session if a pretrained model isn't provided\n",
    "        \n",
    "    elif load_model == False:\n",
    "        sess = session\n",
    "\n",
    "    game.set_sound_enabled(True)\n",
    "    episode_rewards = list()\n",
    "    \n",
    "#Avoid reinitializing the game if this was already done by the training process\n",
    "    \n",
    "    if training == False:\n",
    "        game.init()\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        game.new_episode()\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            \n",
    "            if depth == False:\n",
    "                depth_buffer = np.zeros(state.screen_buffer[:, :, :1].shape)\n",
    "            else:\n",
    "                depth_buffer = np.expand_dims(state.depth_buffer, axis=2)\n",
    "                \n",
    "            buffer = np.concatenate((state.screen_buffer, depth_buffer), axis=2)\n",
    "            state1 = preprocess(buffer, down_sample_ratio)\n",
    "            action = model.choose_action(sess, state1)[0]\n",
    "            reward = game.make_action(actions[action])\n",
    "            \n",
    "#Add a delay between each time step so that the episodes occur at normal speed\n",
    "\n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        episode_rewards.append(game.get_total_reward())\n",
    "        print('Test Episode {} Reward: {}'.format(i + 1, game.get_total_reward()))\n",
    "        time.sleep(1)\n",
    "    \n",
    "#Avoid ending the game so that the training process can continue\n",
    "    \n",
    "    if training == False:\n",
    "        game.close()\n",
    "    \n",
    "    return np.mean(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-27T15:43:12.593Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a Q-network to estimate values and choose actions for a given state\n",
    "\n",
    "class Q_network():\n",
    "    def __init__(self, network_name, height, width, channels, learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.s_t = tf.placeholder(tf.float32,\n",
    "                                  shape=[None, height, width, channels],\n",
    "                                  name=network_name + '_state'\n",
    "                                 )\n",
    "        self.a_t = tf.placeholder(tf.int32,\n",
    "                                  shape=[None],\n",
    "                                  name=network_name + '_action'\n",
    "                                 )\n",
    "        self.Q_target = tf.placeholder(tf.float32,\n",
    "                                       shape=[None, num_actions],\n",
    "                                       name=network_name + '_Q_target'\n",
    "                                      )\n",
    "\n",
    "        self.input_layer = tf.reshape(self.s_t,\n",
    "                                      [-1, height, width, channels],\n",
    "                                      name=network_name + '_input_layer'\n",
    "                                     )\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input_layer,\n",
    "                                      filters=32,\n",
    "                                      kernel_size=[8, 8],\n",
    "                                      strides=[4, 4],\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name=network_name + '_conv1_layer'\n",
    "                                     )\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
    "                                      filters=64,\n",
    "                                      kernel_size=[4, 4],\n",
    "                                      strides=[2, 2],\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name=network_name + '_conv2_layer'\n",
    "                                     )\n",
    "        self.flatten = tf.reshape(self.conv2,\n",
    "                                  [-1, 6*8*64],\n",
    "                                  name=network_name + '_flatten'\n",
    "                                 )\n",
    "        self.dense = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=512,\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name=network_name + '_dense1_layer'\n",
    "                                    )\n",
    "        self.Q_values = tf.layers.dense(inputs=self.dense,\n",
    "                                        units=len(actions),\n",
    "                                        activation=None,\n",
    "                                        name=network_name + '_output_layer'\n",
    "                                       )        \n",
    "    \n",
    "        self.best_action = tf.argmax(self.Q_values, 1)\n",
    "        self.loss = tf.losses.mean_squared_error(self.Q_values,\n",
    "                                                 self.Q_target)\n",
    "        self.adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate,\n",
    "                                           name=network_name + '_adam'\n",
    "                                          )\n",
    "        self.train = self.adam.minimize(self.loss)\n",
    "        \n",
    "    def update_lr(self, epoch):\n",
    "        self.learning_rate = 0.98*self.learning_rate\n",
    "\n",
    "    def calculate_loss(self, session, s, q):\n",
    "        L, _ = session.run([self.loss, self.train],\n",
    "                           feed_dict={self.s_t: s,\n",
    "                                      self.Q_target: q})\n",
    "    \n",
    "        return L\n",
    "\n",
    "#Return the array of Q-values and the best action associated with a given state\n",
    "\n",
    "    def get_Q_values(self, session, s):\n",
    "        Q = session.run(self.Q_values,\n",
    "                        feed_dict={self.s_t: s})\n",
    "\n",
    "        return Q\n",
    "    \n",
    "    def choose_action(self, session, s):\n",
    "        a = session.run(self.best_action,\n",
    "                        feed_dict={self.s_t: s})\n",
    "    \n",
    "        return a\n",
    "    \n",
    "#Create a list of variable update operations\n",
    "\n",
    "def update_graph(variables):\n",
    "    update_ops = list()\n",
    "    \n",
    "#Assign weight values from the network created first to the one created second\n",
    "    \n",
    "    for idx, variable in enumerate(variables[:len(variables)//2]):\n",
    "        op = variable.assign(variables[idx + len(variables)//2].value())\n",
    "        update_ops.append(op)\n",
    "    \n",
    "    return update_ops\n",
    "\n",
    "#Update the target network parameters to match those of the online network\n",
    "\n",
    "def update_target(ops, session):\n",
    "    for op in update_ops:\n",
    "        session.run(op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-27T15:43:12.595Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:17<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Mean Reward: -70.6726957321167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:46<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Mean Reward: -70.51831574249267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:05<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Mean Reward: -69.86438262176513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:29<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Mean Reward: -70.020674949646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:48<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Mean Reward: -71.2573369140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:08<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Mean Reward: -66.79126322937012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [10:32<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Mean Reward: -67.11312698364257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [10:46<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Mean Reward: -69.02893545532227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:29<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Mean Reward: -68.82013133239747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [10:10<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Mean Reward: -68.38439338684083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:06<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Mean Reward: -68.5786576385498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:05<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Mean Reward: -70.42083762359619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:09<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Mean Reward: -69.89468951416016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:54<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Mean Reward: -70.43417224121093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:20<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Mean Reward: -68.55143242645264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:25<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Mean Reward: -68.19747480010986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:20<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Mean Reward: -68.63987706756592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:33<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Mean Reward: -68.02025023651123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:32<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Mean Reward: -67.95362667846679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:50<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Mean Reward: -66.40541453552247\n",
      "Epoch 20 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 20 test with depth buffer:\n",
      "Test Episode 1 Reward: 72.81971740722656\n",
      "Test Episode 2 Reward: 80.98606872558594\n",
      "Test Episode 3 Reward: 80.98606872558594\n",
      "Test Episode 4 Reward: 93.06916809082031\n",
      "Test Episode 5 Reward: 38.58656311035156\n",
      "Test Episode 6 Reward: 27.776565551757812\n",
      "Test Episode 7 Reward: 138.99075317382812\n",
      "Test Episode 8 Reward: 80.98606872558594\n",
      "Test Episode 9 Reward: 80.98606872558594\n",
      "Test Episode 10 Reward: 58.32145690917969\n",
      "Average Test Reward (with depth buffer:) 75.35084991455078\n",
      "Epoch 20 test without depth buffer:\n",
      "Test Episode 1 Reward: -8.509994506835938\n",
      "Test Episode 2 Reward: -8.509994506835938\n",
      "Test Episode 3 Reward: -3.74169921875\n",
      "Test Episode 4 Reward: -40.842315673828125\n",
      "Test Episode 5 Reward: -56.33369445800781\n",
      "Test Episode 6 Reward: -8.509994506835938\n",
      "Test Episode 7 Reward: -8.509994506835938\n",
      "Test Episode 8 Reward: -8.509994506835938\n",
      "Test Episode 9 Reward: -8.509994506835938\n",
      "Test Episode 10 Reward: -26.544296264648438\n",
      "Average Test Reward (without depth buffer): -17.852197265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:40<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Mean Reward: -68.15007958221436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:17<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Mean Reward: -69.43667860412597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:34<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Mean Reward: -68.82995029449462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:56<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Mean Reward: -69.04939730072022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:03<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Mean Reward: -66.49851142883301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:54<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Mean Reward: -68.87528173828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:44<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Mean Reward: -68.64050068664551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:54<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Mean Reward: -69.44159510040284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:15<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Mean Reward: -69.31050761413574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:18<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Mean Reward: -66.89913606262208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:02<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Mean Reward: -67.21548526000977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:33<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Mean Reward: -70.91343824005126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:15<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Mean Reward: -68.74758985137939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:18<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Mean Reward: -68.23819803619385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:11<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Mean Reward: -66.80223712921142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:26<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Mean Reward: -68.08989212036133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:45<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Mean Reward: -67.62672456359863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:10<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Mean Reward: -67.56650213623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:43<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Mean Reward: -70.07744355773926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:47<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Mean Reward: -69.00820126342774\n",
      "Epoch 40 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 40 test with depth buffer:\n",
      "Test Episode 1 Reward: 131.26002502441406\n",
      "Test Episode 2 Reward: 389.7648620605469\n",
      "Test Episode 3 Reward: 143.1270751953125\n",
      "Test Episode 4 Reward: 95.76017761230469\n",
      "Test Episode 5 Reward: 214.9745635986328\n",
      "Test Episode 6 Reward: 214.9745635986328\n",
      "Test Episode 7 Reward: 214.9745635986328\n",
      "Test Episode 8 Reward: 113.3040771484375\n",
      "Test Episode 9 Reward: 214.9745635986328\n",
      "Test Episode 10 Reward: 214.9745635986328\n",
      "Average Test Reward (with depth buffer:) 194.80890350341798\n",
      "Epoch 40 test without depth buffer:\n",
      "Test Episode 1 Reward: 51.66612243652344\n",
      "Test Episode 2 Reward: 53.724151611328125\n",
      "Test Episode 3 Reward: -15.699172973632812\n",
      "Test Episode 4 Reward: 51.66612243652344\n",
      "Test Episode 5 Reward: -69.23631286621094\n",
      "Test Episode 6 Reward: 215.8292694091797\n",
      "Test Episode 7 Reward: 50.9044189453125\n",
      "Test Episode 8 Reward: 51.66612243652344\n",
      "Test Episode 9 Reward: 58.123291015625\n",
      "Test Episode 10 Reward: 51.66612243652344\n",
      "Average Test Reward (without depth buffer): 50.03101348876953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:08<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Mean Reward: -68.06334611511231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:05<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Mean Reward: -68.39065028381347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:27<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Mean Reward: -69.71428273010254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:42<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Mean Reward: -66.97016018676757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:25<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Mean Reward: -65.73708443450927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:24<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Mean Reward: -66.82600158691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:22<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Mean Reward: -70.09662278747558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:28<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Mean Reward: -68.10399378967286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:11<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Mean Reward: -70.26858459472656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:11<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Mean Reward: -68.49530377197266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:32<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Mean Reward: -69.56222290039062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:03<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Mean Reward: -68.90293083953857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:55<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Mean Reward: -68.12421102905273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:35<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Mean Reward: -68.76908132171631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:47<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Mean Reward: -69.62763373565674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:01<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Mean Reward: -66.24878966522216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:49<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Mean Reward: -68.43659350585938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:54<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Mean Reward: -67.78063638305665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:02<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Mean Reward: -68.57988148498535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:25<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Mean Reward: -70.5367495880127\n",
      "Epoch 60 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 60 test with depth buffer:\n",
      "Test Episode 1 Reward: 112.24687194824219\n",
      "Test Episode 2 Reward: 88.91380310058594\n",
      "Test Episode 3 Reward: 88.91380310058594\n",
      "Test Episode 4 Reward: 88.91380310058594\n",
      "Test Episode 5 Reward: 88.91380310058594\n",
      "Test Episode 6 Reward: 87.94873046875\n",
      "Test Episode 7 Reward: 117.29924011230469\n",
      "Test Episode 8 Reward: 370.2001190185547\n",
      "Test Episode 9 Reward: 360.17041015625\n",
      "Test Episode 10 Reward: 355.3059844970703\n",
      "Average Test Reward (with depth buffer:) 175.88265686035157\n",
      "Epoch 60 test without depth buffer:\n",
      "Test Episode 1 Reward: -16.601882934570312\n",
      "Test Episode 2 Reward: 89.10957336425781\n",
      "Test Episode 3 Reward: 53.10284423828125\n",
      "Test Episode 4 Reward: -19.413223266601562\n",
      "Test Episode 5 Reward: 53.10284423828125\n",
      "Test Episode 6 Reward: 53.10284423828125\n",
      "Test Episode 7 Reward: -18.851913452148438\n",
      "Test Episode 8 Reward: -93.05158996582031\n",
      "Test Episode 9 Reward: 53.10284423828125\n",
      "Test Episode 10 Reward: 9.5679931640625\n",
      "Average Test Reward (without depth buffer): 16.31703338623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:02<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Mean Reward: -69.34668611907959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:55<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Mean Reward: -68.11359796142578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:41<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Mean Reward: -67.78433435058594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:11<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Mean Reward: -66.13114820861816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:32<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Mean Reward: -68.22951669311523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:11<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Mean Reward: -68.3294031677246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:56<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Mean Reward: -68.60973760986329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:01<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Mean Reward: -69.02881126403808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:26<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Mean Reward: -70.52694003295899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:18<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Mean Reward: -67.78952525329589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:23<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Mean Reward: -66.63819975280762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:38<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Mean Reward: -67.92739408874512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:18<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Mean Reward: -68.67475106811523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:35<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Mean Reward: -68.63835050964356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:53<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Mean Reward: -69.75699773406983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:45<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Mean Reward: -68.9784730682373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:39<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Mean Reward: -69.38364220428467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:48<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Mean Reward: -69.72841886138916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:20<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Mean Reward: -69.24781019592285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:21<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Mean Reward: -68.56046757507325\n",
      "Epoch 80 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 80 test with depth buffer:\n",
      "Test Episode 1 Reward: 217.76788330078125\n",
      "Test Episode 2 Reward: 217.76788330078125\n",
      "Test Episode 3 Reward: 79.38572692871094\n",
      "Test Episode 4 Reward: 13.647445678710938\n",
      "Test Episode 5 Reward: 105.68707275390625\n",
      "Test Episode 6 Reward: 217.76788330078125\n",
      "Test Episode 7 Reward: 172.5204315185547\n",
      "Test Episode 8 Reward: 369.8952941894531\n",
      "Test Episode 9 Reward: 294.5778350830078\n",
      "Test Episode 10 Reward: 217.76788330078125\n",
      "Average Test Reward (with depth buffer:) 190.67853393554688\n",
      "Epoch 80 test without depth buffer:\n",
      "Test Episode 1 Reward: -1.6921234130859375\n",
      "Test Episode 2 Reward: 84.44912719726562\n",
      "Test Episode 3 Reward: -1.6921234130859375\n",
      "Test Episode 4 Reward: 42.67054748535156\n",
      "Test Episode 5 Reward: -1.6921234130859375\n",
      "Test Episode 6 Reward: -1.6921234130859375\n",
      "Test Episode 7 Reward: -1.6921234130859375\n",
      "Test Episode 8 Reward: -1.6921234130859375\n",
      "Test Episode 9 Reward: -1.6921234130859375\n",
      "Test Episode 10 Reward: -1.6921234130859375\n",
      "Average Test Reward (without depth buffer): 11.358268737792969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:26<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Mean Reward: -65.07257484436035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:02<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Mean Reward: -67.09698007965088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:47<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Mean Reward: -68.06963017272949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:42<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Mean Reward: -68.03128240203857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:44<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Mean Reward: -67.29884914398194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:34<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Mean Reward: -71.21670111846923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:40<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Mean Reward: -67.8625856552124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:41<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Mean Reward: -69.1391785736084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:51<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Mean Reward: -68.13406923675537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:46<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Mean Reward: -67.97279904174805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:39<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Mean Reward: -68.32521184539794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:38<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Mean Reward: -69.62936206817626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:48<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Mean Reward: -69.27470381164551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:40<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Mean Reward: -69.58446295928955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:42<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Mean Reward: -68.64348641204835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:59<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Mean Reward: -68.59017743682861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:29<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Mean Reward: -68.94331882476807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:30<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Mean Reward: -68.52298240661621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:20<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Mean Reward: -70.28805194091797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:30<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Mean Reward: -66.98542767333984\n",
      "Epoch 100 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 100 test with depth buffer:\n",
      "Test Episode 1 Reward: 63.26948547363281\n",
      "Test Episode 2 Reward: 74.81126403808594\n",
      "Test Episode 3 Reward: 74.81126403808594\n",
      "Test Episode 4 Reward: 74.81126403808594\n",
      "Test Episode 5 Reward: 224.06375122070312\n",
      "Test Episode 6 Reward: 74.81126403808594\n",
      "Test Episode 7 Reward: 52.862945556640625\n",
      "Test Episode 8 Reward: 74.81126403808594\n",
      "Test Episode 9 Reward: 74.81126403808594\n",
      "Test Episode 10 Reward: 59.23204040527344\n",
      "Average Test Reward (with depth buffer:) 84.82958068847657\n",
      "Epoch 100 test without depth buffer:\n",
      "Test Episode 1 Reward: 186.7234344482422\n",
      "Test Episode 2 Reward: 7.160430908203125\n",
      "Test Episode 3 Reward: -6.4378814697265625\n",
      "Test Episode 4 Reward: 201.58114624023438\n",
      "Test Episode 5 Reward: 186.7234344482422\n",
      "Test Episode 6 Reward: -17.00762939453125\n",
      "Test Episode 7 Reward: -22.245590209960938\n",
      "Test Episode 8 Reward: 671.1471099853516\n",
      "Test Episode 9 Reward: 24.471908569335938\n",
      "Test Episode 10 Reward: 186.7234344482422\n",
      "Average Test Reward (without depth buffer): 141.88397979736328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:19<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101 Mean Reward: -66.24993885040283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:26<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102 Mean Reward: -66.9095492401123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:14<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103 Mean Reward: -70.03994078063965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:25<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104 Mean Reward: -69.0269677734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:04<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105 Mean Reward: -71.7968417892456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:08<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106 Mean Reward: -68.56792385864257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:15<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 Mean Reward: -67.9547851486206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:09<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 Mean Reward: -66.61466916656494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:11<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109 Mean Reward: -67.21061940002441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:37<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110 Mean Reward: -70.3744309387207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:27<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111 Mean Reward: -67.81602911376953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:14<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112 Mean Reward: -67.84620623779297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:59<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113 Mean Reward: -69.6560033416748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:14<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114 Mean Reward: -70.39423426818847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:00<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115 Mean Reward: -67.48122008514405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:53<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116 Mean Reward: -68.7779677658081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:47<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117 Mean Reward: -68.48710897064208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:47<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 Mean Reward: -68.49768998718261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:38<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119 Mean Reward: -67.68123135375977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:38<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120 Mean Reward: -69.61175968170166\n",
      "Epoch 120 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 120 test with depth buffer:\n",
      "Test Episode 1 Reward: 371.0050811767578\n",
      "Test Episode 2 Reward: 214.73204040527344\n",
      "Test Episode 3 Reward: 503.50746154785156\n",
      "Test Episode 4 Reward: 214.73204040527344\n",
      "Test Episode 5 Reward: 673.5232238769531\n",
      "Test Episode 6 Reward: 25.202438354492188\n",
      "Test Episode 7 Reward: 214.73204040527344\n",
      "Test Episode 8 Reward: 810.4558868408203\n",
      "Test Episode 9 Reward: 304.0842742919922\n",
      "Test Episode 10 Reward: 331.60894775390625\n",
      "Average Test Reward (with depth buffer:) 366.35834350585935\n",
      "Epoch 120 test without depth buffer:\n",
      "Test Episode 1 Reward: 3.3960418701171875\n",
      "Test Episode 2 Reward: 94.23623657226562\n",
      "Test Episode 3 Reward: 94.23623657226562\n",
      "Test Episode 4 Reward: 94.23623657226562\n",
      "Test Episode 5 Reward: 94.23623657226562\n",
      "Test Episode 6 Reward: 94.23623657226562\n",
      "Test Episode 7 Reward: 45.68034362792969\n",
      "Test Episode 8 Reward: 13.14520263671875\n",
      "Test Episode 9 Reward: 94.23623657226562\n",
      "Test Episode 10 Reward: 94.23623657226562\n",
      "Average Test Reward (without depth buffer): 72.1875244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:49<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121 Mean Reward: -68.13826595306396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:46<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122 Mean Reward: -68.81862616729737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:45<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123 Mean Reward: -66.86200967407227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:29<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124 Mean Reward: -67.3159875869751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:00<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125 Mean Reward: -68.64147190856933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:17<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126 Mean Reward: -68.73470854187012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:21<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127 Mean Reward: -66.77578329467774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:04<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 Mean Reward: -68.20147066497803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:04<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 Mean Reward: -69.88412196350097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:38<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130 Mean Reward: -69.46361573791503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:16<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131 Mean Reward: -68.97342418670654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:50<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132 Mean Reward: -67.84795471191406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:39<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133 Mean Reward: -66.96455383300781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:00<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134 Mean Reward: -68.50965341186523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:08<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135 Mean Reward: -67.6869645767212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:52<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136 Mean Reward: -69.04148820495605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:03<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137 Mean Reward: -69.192563331604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:50<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138 Mean Reward: -69.2476470565796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:20<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 Mean Reward: -69.81557570648194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:14<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 Mean Reward: -68.12921504974365\n",
      "Epoch 140 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 140 test with depth buffer:\n",
      "Test Episode 1 Reward: 89.51968383789062\n",
      "Test Episode 2 Reward: 89.51968383789062\n",
      "Test Episode 3 Reward: 80.11625671386719\n",
      "Test Episode 4 Reward: 50.90382385253906\n",
      "Test Episode 5 Reward: 89.51968383789062\n",
      "Test Episode 6 Reward: 89.51968383789062\n",
      "Test Episode 7 Reward: 105.89729309082031\n",
      "Test Episode 8 Reward: 441.5984649658203\n",
      "Test Episode 9 Reward: 89.51968383789062\n",
      "Test Episode 10 Reward: 89.51968383789062\n",
      "Average Test Reward (with depth buffer:) 121.56339416503906\n",
      "Epoch 140 test without depth buffer:\n",
      "Test Episode 1 Reward: 192.67703247070312\n",
      "Test Episode 2 Reward: 69.44046020507812\n",
      "Test Episode 3 Reward: 273.7863006591797\n",
      "Test Episode 4 Reward: 17.582778930664062\n",
      "Test Episode 5 Reward: 192.67703247070312\n",
      "Test Episode 6 Reward: 192.67703247070312\n",
      "Test Episode 7 Reward: 192.67703247070312\n",
      "Test Episode 8 Reward: -4.985198974609375\n",
      "Test Episode 9 Reward: 192.67703247070312\n",
      "Test Episode 10 Reward: 192.67703247070312\n",
      "Average Test Reward (without depth buffer): 151.18865356445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:19<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141 Mean Reward: -69.63718578338623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:17<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142 Mean Reward: -68.53458796691895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:16<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143 Mean Reward: -68.17916548919678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:22<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144 Mean Reward: -68.58955062103271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:24<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145 Mean Reward: -69.44238436126709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:28<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146 Mean Reward: -66.9170004196167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:48<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147 Mean Reward: -69.02110375213623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:17<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148 Mean Reward: -68.30039226531983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:20<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 Mean Reward: -69.61234588623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:11<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 Mean Reward: -69.2189310684204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:08<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151 Mean Reward: -37.807291244506835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:21<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152 Mean Reward: -42.11442250823975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:24<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153 Mean Reward: -38.50582051849365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:14<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154 Mean Reward: -38.78905511474609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:15<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155 Mean Reward: -38.851295265197756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:21<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156 Mean Reward: -35.30804943084717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:20<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157 Mean Reward: -35.611233116149904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:20<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158 Mean Reward: -35.55910134887695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:25<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159 Mean Reward: -37.918340156555175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:11<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 Mean Reward: -33.8279142074585\n",
      "Epoch 160 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "Epoch 160 test with depth buffer:\n",
      "Test Episode 1 Reward: 71.63063049316406\n",
      "Test Episode 2 Reward: 71.63063049316406\n",
      "Test Episode 3 Reward: 64.2850341796875\n",
      "Test Episode 4 Reward: 366.63433837890625\n",
      "Test Episode 5 Reward: 71.63063049316406\n",
      "Test Episode 6 Reward: 396.05320739746094\n",
      "Test Episode 7 Reward: 357.04522705078125\n",
      "Test Episode 8 Reward: 87.20166015625\n",
      "Test Episode 9 Reward: 39.20991516113281\n",
      "Test Episode 10 Reward: 71.63063049316406\n",
      "Average Test Reward (with depth buffer:) 159.6951904296875\n",
      "Epoch 160 test without depth buffer:\n",
      "Test Episode 1 Reward: 80.19996643066406\n",
      "Test Episode 2 Reward: 199.57960510253906\n",
      "Test Episode 3 Reward: 243.65679931640625\n",
      "Test Episode 4 Reward: 288.17955017089844\n",
      "Test Episode 5 Reward: -12.0894775390625\n",
      "Test Episode 6 Reward: 199.57960510253906\n",
      "Test Episode 7 Reward: 199.57960510253906\n",
      "Test Episode 8 Reward: 350.77549743652344\n",
      "Test Episode 9 Reward: 226.4566192626953\n",
      "Test Episode 10 Reward: 199.57960510253906\n",
      "Average Test Reward (without depth buffer): 197.54973754882812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:57<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 Mean Reward: -34.52717741394043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:53<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162 Mean Reward: -35.02463423156738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:10<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163 Mean Reward: -34.23823212432861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:10<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164 Mean Reward: -35.098093444824215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:14<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165 Mean Reward: -34.35420431518555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:14<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166 Mean Reward: -30.56830850982666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:06<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167 Mean Reward: -33.57990663909912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:10<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168 Mean Reward: -31.646459915161135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:11<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169 Mean Reward: -30.385175041198732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:11<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170 Mean Reward: -32.18021639251709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:18<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 Mean Reward: -27.667896865844728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:09<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 Mean Reward: -27.685209465026855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:05<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173 Mean Reward: -29.92675695800781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 1311/2000 [02:04<01:05, 10.56it/s]"
     ]
    }
   ],
   "source": [
    "#For each time step, collect the following data:\n",
    "#The current game state\n",
    "#The action that was taken taken\n",
    "#The reward obtained from the chosen action\n",
    "#The next game state (store the first game state if the previous action ends the episode)\n",
    "#A variable indicating whether the episode is over yet\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Instantiate the target network before the online network so that it's updated correctly\n",
    "\n",
    "target_net = Q_network(network_name='target',\n",
    "                       learning_rate=learning_rate,\n",
    "                       height=height,\n",
    "                       width=width,\n",
    "                       channels=channels)\n",
    "DQN = Q_network(network_name='online',\n",
    "                learning_rate=learning_rate,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                channels=channels)\n",
    "\n",
    "exp_buffer = Buffer(size=buffer_size)\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver(max_to_keep=10, reshape=True)\n",
    "weights = tf.trainable_variables()\n",
    "\n",
    "update_ops = update_graph(weights)\n",
    "\n",
    "if load_model == True:\n",
    "    print('Loading model from', model_dir)\n",
    "    tf.train.Saver().restore(session, model_dir)\n",
    "    \n",
    "elif load_model == False:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "game.set_sound_enabled(False)\n",
    "game.init()\n",
    "\n",
    "t = 0\n",
    "epoch_rank = list()\n",
    "\n",
    "#Accumulate experiences in the buffer using an epsilon-greedy strategy with three training phases\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_rewards = list()\n",
    "    \n",
    "    for step in trange(steps_per_epoch, leave=True):\n",
    "        experience = list()\n",
    "        game.new_episode()\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            \n",
    "#Substitute an array of zeros for the depth buffer if that setting is disabled\n",
    "\n",
    "            if game.is_depth_buffer_enabled() == False:\n",
    "                depth_buffer = np.zeros(state.screen_buffer[:, :, :1].shape)\n",
    "            else:\n",
    "                depth_buffer = np.expand_dims(state.depth_buffer, axis=2)\n",
    "            \n",
    "            state1 = preprocess(np.concatenate((state.screen_buffer,\n",
    "                                                depth_buffer),\n",
    "                                                axis=2),\n",
    "                                               down_sample_ratio)\n",
    "            \n",
    "#Explore the environment by choosing random actions with 100% probability for the first phase of training\n",
    "\n",
    "            if epoch < 0.3*epochs:\n",
    "                action = np.random.randint(num_actions)\n",
    "            \n",
    "#Increase the probability of greedily choosing an action by a constant amount at each epoch in the second phase\n",
    "            \n",
    "            elif epoch < 0.9*epochs:\n",
    "                epsilon = start_epsilon - (epoch + 1 - 0.2*epochs)*(start_epsilon-end_epsilon)/(0.7*epochs)\n",
    "            \n",
    "                if np.random.uniform(0, 1) <= epsilon:\n",
    "                    action = np.random.randint(num_actions)\n",
    "                else:\n",
    "                    action = DQN.choose_action(session, state1)[0]\n",
    "\n",
    "#Select a random action with 10% probability in the final phase of training\n",
    "                \n",
    "            else:\n",
    "                if np.random.uniform(0, 1) <= end_epsilon:\n",
    "                    action = np.random.randint(num_actions)\n",
    "                else:\n",
    "                    action = DQN.choose_action(session, state1)[0]\n",
    "\n",
    "            reward = game.make_action(actions[action], frame_delay)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done == False:\n",
    "                state = game.get_state()\n",
    "                state2 = preprocess(np.concatenate((state.screen_buffer,\n",
    "                                                    depth_buffer),\n",
    "                                                    axis=2),\n",
    "                                                    down_sample_ratio)\n",
    "            elif done == True:\n",
    "                state2 = state1\n",
    "        \n",
    "#Add the experience obtained from each time step to the buffer\n",
    "\n",
    "            t += 1\n",
    "            exp_buffer.add_experience((state1, action, reward, state2, done))\n",
    "        \n",
    "#Sample a minibatch from the buffer if there are enough experiences in the buffer\n",
    "\n",
    "        if exp_buffer.length > batch_size:\n",
    "            s1, a, r, s2, terminal = exp_buffer.sample_buffer(batch_size)\n",
    "            \n",
    "#Get the target values from the target Q-network\n",
    "            \n",
    "            target_Q = np.max(target_net.get_Q_values(session, s2), axis=1)\n",
    "            \n",
    "#Train the online Q-network by using a minibatch to update the action-value function\n",
    "            \n",
    "            Q2 = DQN.get_Q_values(session, s1)\n",
    "            Q2[np.arange(batch_size), a] = r + gamma*(1 - terminal)*target_Q\n",
    "            DQN.calculate_loss(session, s1, Q2)\n",
    "            \n",
    "        epoch_rewards.append(game.get_total_reward())\n",
    "        \n",
    "#Increase the discount factor at each epoch until it reaches 0.99\n",
    "    \n",
    "    if gamma < 0.99:\n",
    "        gamma = 1-.98*(1-gamma)\n",
    "    elif gamma >= 0.99:\n",
    "        gamma = 0.99\n",
    "        \n",
    "#Decrease the learning rate at each epoch\n",
    "\n",
    "    DQN.update_lr(epoch)\n",
    "    target_net.update_lr(epoch)\n",
    "    \n",
    "    print('Epoch {} Mean Reward: {}'.format(epoch + 1, np.mean(epoch_rewards)))\n",
    "    \n",
    "#Save the model, update the target network, and test the agent for 10 episodes every 20 epochs\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0 and epoch > 0:\n",
    "        if save_model == True:\n",
    "            checkpoint = model_dir + '-' + str(epoch + 1)\n",
    "            print('Epoch {} Model saved to {}'.format(epoch + 1, model_dir))\n",
    "            saver.save(session, model_dir, global_step=epoch + 1)\n",
    "            \n",
    "        update_target(update_ops, session)\n",
    "\n",
    "#Test the agent both with and without the depth buffer given\n",
    "        \n",
    "        print('Epoch {} test with depth buffer:'.format(epoch + 1))\n",
    "        test_reward_depth = test_agent(DQN, num_episodes=10,\n",
    "                                       training=True,\n",
    "                                       load_model=False,\n",
    "                                       depth=True,\n",
    "                                       session=session,\n",
    "                                       model_dir=model_dir)\n",
    "        print('Average Test Reward (with depth buffer:)', test_reward_depth)\n",
    "        \n",
    "        print('Epoch {} test without depth buffer:'.format(epoch + 1))\n",
    "        test_reward = test_agent(DQN, num_episodes=10,\n",
    "                                 training=True,\n",
    "                                 load_model=False,\n",
    "                                 depth=False,\n",
    "                                 session=session,\n",
    "                                 model_dir=model_dir)\n",
    "        print('Average Test Reward (without depth buffer):', test_reward)\n",
    "        \n",
    "        epoch_rank.append((test_reward, epoch + 1))\n",
    "        \n",
    "#Return a sorted list of epoch checkpoints based on average test episode reward\n",
    "        \n",
    "print(sorted(epoch_rank, reverse=True))\n",
    "print('{} time steps experienced during training'.format(t))\n",
    "game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-27T15:43:12.597Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get a list of checkpoints saved during training\n",
    "\n",
    "ckpts = tf.train.get_checkpoint_state('checkpoints').all_model_checkpoint_paths\n",
    "\n",
    "#Test the trained model from a certain checkpoint by only choosing actions with a greedy strategy\n",
    "\n",
    "test_reward = test_agent(DQN, num_episodes=20,\n",
    "                         training=False,\n",
    "                         load_model=True,\n",
    "                         depth=True,\n",
    "                         model_dir=ckpts[-1])\n",
    "print('Average Test Reward (with depth buffer):', test_reward)\n",
    "\n",
    "test_reward = test_agent(DQN, num_episodes=20,\n",
    "                         training=False,\n",
    "                         load_model=True,\n",
    "                         depth=False,\n",
    "                         model_dir=ckpts[-1])\n",
    "print('Average Test Reward (without depth buffer):', test_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
