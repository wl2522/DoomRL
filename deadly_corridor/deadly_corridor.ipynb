{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T16:23:38.765146Z",
     "start_time": "2018-01-19T16:23:33.672628Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import scipy.misc\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import trange\n",
    "from IPython.display import HTML\n",
    "\n",
    "#Import the vizdoom package as \"vd\" since it can't be installed normally on Windows\n",
    "\n",
    "vd_location = 'C:/Anaconda3/envs/doom/Lib/site-packages/vizdoom/vizdoom.pyd'\n",
    "vizdoom = importlib.util.spec_from_file_location('vizdoom',\n",
    "                                                 vd_location)\n",
    "vd = importlib.util.module_from_spec(vizdoom)\n",
    "vizdoom.loader.exec_module(vd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T16:23:38.950825Z",
     "start_time": "2018-01-19T16:23:38.766648Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Specify the game scenario and the screen format/resolution\n",
    "\n",
    "game = vd.DoomGame()\n",
    "game.set_screen_format(vd.ScreenFormat.BGR24)\n",
    "game.set_depth_buffer_enabled(True)\n",
    "game.set_screen_resolution(vd.ScreenResolution.RES_160X120)\n",
    "game.load_config('deadly_corridor.cfg')\n",
    "\n",
    "down_sample_ratio = 0.5\n",
    "width = int(game.get_screen_width()*down_sample_ratio)\n",
    "height = int(game.get_screen_height()*down_sample_ratio)\n",
    "channels = game.get_screen_channels() + 1\n",
    "\n",
    "#Specify the available actions in the scenario\n",
    "\n",
    "available_actions = game.get_available_buttons()\n",
    "actions = [list(ohe) for ohe in list(np.identity(len(available_actions)))]\n",
    "num_actions = len(available_actions)\n",
    "\n",
    "#Specify the Q-network learning parameters\n",
    "\n",
    "frame_delay = 12\n",
    "buffer_size = 50000\n",
    "epochs = 80\n",
    "steps_per_epoch = 2000\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.001\n",
    "start_epsilon = 1.0\n",
    "end_epsilon = 0.1\n",
    "batch_size = 100\n",
    "load_model = False\n",
    "save_model = True\n",
    "model_dir = './checkpoints/deadly_corridor.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T16:23:39.269133Z",
     "start_time": "2018-01-19T16:23:38.951827Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a buffer object that holds a set of training experiences (state-action-reward tuples)\n",
    "\n",
    "class Buffer():\n",
    "    def __init__(self, size=1000):\n",
    "        self.buffer = list()\n",
    "        self.length = len(self.buffer)\n",
    "        self.size = size\n",
    "        \n",
    "#Add a new experience to the buffer (remove the oldest experience if the buffer is already full)\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        if self.length + 1 >= self.size:\n",
    "            self.buffer[0:(self.length + 1) - self.size] = []\n",
    "        \n",
    "        self.buffer.append(experience)\n",
    "        self.length = len(self.buffer)\n",
    "            \n",
    "#Return a batch of experience arrays randomly sampled from the buffer\n",
    "            \n",
    "    def sample_buffer(self, sample_size):\n",
    "        sample = np.random.randint(self.length, size=sample_size)\n",
    "        s1 = np.concatenate([self.buffer[idx][0] for idx in sample], axis=0)\n",
    "        a = np.array([self.buffer[idx][1] for idx in sample])\n",
    "        r = np.array([self.buffer[idx][2] for idx in sample])\n",
    "        s2 = np.concatenate([self.buffer[idx][3] for idx in sample], axis=0)\n",
    "        terminal = np.array([self.buffer[idx][4] for idx in sample], dtype=np.int32)\n",
    "        \n",
    "        return s1, a, r, s2, terminal\n",
    "\n",
    "#Downsample and normalize an image array representing the game state at a given time stamp\n",
    "\n",
    "def preprocess(image, down_sample_ratio=1):\n",
    "    if down_sample_ratio != 1:\n",
    "        image = scipy.misc.imresize(image, down_sample_ratio)\n",
    "    image = image.astype(np.float32)\n",
    "    image /= 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "#Test the agent using a currently training or previously trained model\n",
    "\n",
    "def test_agent(model, num_episodes, load_model, training=True, session=None, model_dir=None):\n",
    "    if load_model == True:\n",
    "        sess = tf.Session()\n",
    "        print('Loading model from', model_dir)\n",
    "        tf.train.Saver().restore(sess, model_dir)\n",
    "        \n",
    "#Require an existing session if a pretrained model isn't provided\n",
    "        \n",
    "    elif load_model == False:\n",
    "        sess = session\n",
    "\n",
    "    game.set_sound_enabled(True)\n",
    "    episode_rewards = list()\n",
    "    \n",
    "#Avoid reinitializing the game if this was already done by the training process\n",
    "    \n",
    "    if training == False:\n",
    "        game.init()\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        game.new_episode()\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            buffer = np.concatenate((state.screen_buffer,\n",
    "                                     np.expand_dims(state.depth_buffer,\n",
    "                                                    axis=2)),\n",
    "                                    axis=2)\n",
    "            state1 = preprocess(buffer, down_sample_ratio)\n",
    "            action = model.choose_action(sess, state1)[0]\n",
    "            reward = game.make_action(actions[action])\n",
    "            \n",
    "#Add a delay between each time step so that the episodes occur at normal speed\n",
    "\n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        episode_rewards.append(game.get_total_reward())\n",
    "        print('Test Episode {} Reward: {}'.format(i + 1, game.get_total_reward()))\n",
    "        time.sleep(1)\n",
    "    \n",
    "#Avoid ending the game so that the training process can continue\n",
    "    \n",
    "    if training == False:\n",
    "        game.close()\n",
    "    \n",
    "    return ('Average Test Reward:', np.mean(episode_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T16:23:39.468826Z",
     "start_time": "2018-01-19T16:23:39.270134Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a Q-network to estimate values and choose actions for a given state\n",
    "\n",
    "class Q_network():\n",
    "    def __init__(self, height, width, channels, learning_rate=0.001):\n",
    "        self.s_t = tf.placeholder(tf.float32,\n",
    "                                  shape=[None, height, width, channels],\n",
    "                                  name='state')\n",
    "        self.a_t = tf.placeholder(tf.int32,\n",
    "                                  shape=[None],\n",
    "                                  name='action')\n",
    "        self.Q_target = tf.placeholder(tf.float32,\n",
    "                                       shape=[None, num_actions],\n",
    "                                       name='Q_target')\n",
    "\n",
    "        self.input_layer = tf.reshape(self.s_t,\n",
    "                                      [-1, height, width, channels],\n",
    "                                      name='input_layer')\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input_layer,\n",
    "                                      filters=32,\n",
    "                                      kernel_size=[8, 8],\n",
    "                                      strides=[4, 4],\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name='conv1_layer')\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
    "                                      filters=64,\n",
    "                                      kernel_size=[4, 4],\n",
    "                                      strides=[2, 2],\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name='conv2_layer')\n",
    "        self.flatten = tf.reshape(self.conv2,\n",
    "                                  [-1, 6*8*64],\n",
    "                                  name='flatten')\n",
    "        self.dense = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=512,\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name='dense1_layer')\n",
    "        self.Q_values = tf.layers.dense(inputs=self.dense,\n",
    "                                        units=len(actions),\n",
    "                                        activation=None,\n",
    "                                        name='output_layer')        \n",
    "    \n",
    "        self.best_action = tf.argmax(self.Q_values, 1)\n",
    "        self.loss = tf.losses.mean_squared_error(self.Q_values,\n",
    "                                                 self.Q_target)\n",
    "        self.adam = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                           name='adam')\n",
    "        self.train = self.adam.minimize(self.loss)\n",
    "\n",
    "    def calculate_loss(self, session, s, q):\n",
    "        L, _ = session.run([self.loss, self.train],\n",
    "                           feed_dict={self.s_t: s,\n",
    "                                      self.Q_target: q})\n",
    "    \n",
    "        return L\n",
    "\n",
    "#Return the array of Q-values and the best action associated with a given state\n",
    "\n",
    "    def get_Q_values(self, session, s):\n",
    "        Q = session.run(self.Q_values,\n",
    "                        feed_dict={self.s_t: s})\n",
    "\n",
    "        return Q\n",
    "    \n",
    "    def choose_action(self, session, s):\n",
    "        a = session.run(self.best_action,\n",
    "                        feed_dict={self.s_t: s})\n",
    "    \n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T16:28:45.723948Z",
     "start_time": "2018-01-19T16:23:39.470328Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Mean Reward: 120.39130401611328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Mean Reward: 154.05533828735352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Mean Reward: 131.62899780273438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Mean Reward: 157.8271598815918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Mean Reward: 119.66508865356445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Mean Reward: 139.97867813110352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Mean Reward: 122.11528778076172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Mean Reward: 130.174365234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Mean Reward: 128.75669708251954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Mean Reward: 142.89727706909179\n",
      "Epoch 10 test:\n",
      "Test Episode 1 Reward: 294.95274353027344\n",
      "Test Episode 2 Reward: 631.3865814208984\n",
      "Test Episode 3 Reward: 300.270751953125\n",
      "Test Episode 4 Reward: 294.95274353027344\n",
      "Test Episode 5 Reward: 294.95274353027344\n",
      "Test Episode 6 Reward: 329.85675048828125\n",
      "Test Episode 7 Reward: 313.27296447753906\n",
      "Test Episode 8 Reward: 294.95274353027344\n",
      "Test Episode 9 Reward: 514.5715789794922\n",
      "Test Episode 10 Reward: 212.5015106201172\n",
      "Epoch 10 Model saved to ./checkpoints/deadly_corridor.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Mean Reward: 131.68692779541016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Mean Reward: 135.68497161865236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Mean Reward: 132.87233047485353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Mean Reward: 121.21613998413086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Mean Reward: 147.86905975341796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Mean Reward: 128.43447189331056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Mean Reward: 150.1641647338867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Mean Reward: 140.14100723266603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Mean Reward: 130.25592803955078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Mean Reward: 145.81566314697267\n",
      "Epoch 20 test:\n",
      "Test Episode 1 Reward: 294.95274353027344\n",
      "Test Episode 2 Reward: 294.95274353027344\n",
      "Test Episode 3 Reward: 294.95274353027344\n",
      "Test Episode 4 Reward: 263.36476135253906\n",
      "Test Episode 5 Reward: 307.1249084472656\n",
      "Test Episode 6 Reward: 294.95274353027344\n",
      "Test Episode 7 Reward: 294.95274353027344\n",
      "Test Episode 8 Reward: 294.95274353027344\n",
      "Test Episode 9 Reward: 294.95274353027344\n",
      "Test Episode 10 Reward: 294.95274353027344\n",
      "Epoch 20 Model saved to ./checkpoints/deadly_corridor.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Mean Reward: 119.53192520141602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Mean Reward: 119.22627334594726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Mean Reward: 121.72506256103516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Mean Reward: 120.31939849853515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Mean Reward: 158.79324111938476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Mean Reward: 142.45037918090821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Mean Reward: 181.48605346679688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Mean Reward: 180.91883697509766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Mean Reward: 133.78098754882814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Mean Reward: 166.66860656738282\n",
      "Epoch 30 test:\n",
      "Test Episode 1 Reward: 294.95274353027344\n",
      "Test Episode 2 Reward: 294.95274353027344\n",
      "Test Episode 3 Reward: 302.1759796142578\n",
      "Test Episode 4 Reward: 294.95274353027344\n",
      "Test Episode 5 Reward: 257.14605712890625\n",
      "Test Episode 6 Reward: 328.3160858154297\n",
      "Test Episode 7 Reward: 294.95274353027344\n",
      "Test Episode 8 Reward: 640.3885650634766\n",
      "Test Episode 9 Reward: 578.2197875976562\n",
      "Test Episode 10 Reward: 294.95274353027344\n",
      "Epoch 30 Model saved to ./checkpoints/deadly_corridor.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Mean Reward: 180.05858840942383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Mean Reward: 178.94553833007814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Mean Reward: 170.79694519042968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Mean Reward: 234.27135467529297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Mean Reward: 213.22036056518556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Mean Reward: 191.69307479858398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Mean Reward: 174.00246658325196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Mean Reward: 184.84504165649415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Mean Reward: 231.9142807006836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Mean Reward: 209.11266326904297\n",
      "Epoch 40 test:\n",
      "Test Episode 1 Reward: 284.82667541503906\n",
      "Test Episode 2 Reward: 284.82667541503906\n",
      "Test Episode 3 Reward: 271.8874206542969\n",
      "Test Episode 4 Reward: 284.82667541503906\n",
      "Test Episode 5 Reward: 639.2828674316406\n",
      "Test Episode 6 Reward: 284.82667541503906\n",
      "Test Episode 7 Reward: 585.1678009033203\n",
      "Test Episode 8 Reward: 284.82667541503906\n",
      "Test Episode 9 Reward: 467.19029235839844\n",
      "Test Episode 10 Reward: 318.2055358886719\n",
      "Epoch 40 Model saved to ./checkpoints/deadly_corridor.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Mean Reward: 267.28471755981445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Mean Reward: 248.21056518554687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Mean Reward: 217.63948974609374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Mean Reward: 213.0647285461426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Mean Reward: 267.9881958007812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Mean Reward: 175.98081130981444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Mean Reward: 219.65376663208008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Mean Reward: 171.19925842285156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Mean Reward: 295.0933242797852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Mean Reward: 197.61941986083986\n",
      "Epoch 50 test:\n",
      "Test Episode 1 Reward: 294.95274353027344\n",
      "Test Episode 2 Reward: 465.12831115722656\n",
      "Test Episode 3 Reward: 294.95274353027344\n",
      "Test Episode 4 Reward: 281.6623840332031\n",
      "Test Episode 5 Reward: 294.95274353027344\n",
      "Test Episode 6 Reward: 294.95274353027344\n",
      "Test Episode 7 Reward: 294.95274353027344\n",
      "Test Episode 8 Reward: 658.4134521484375\n",
      "Test Episode 9 Reward: 294.95274353027344\n",
      "Test Episode 10 Reward: 286.1171569824219\n",
      "Epoch 50 Model saved to ./checkpoints/deadly_corridor.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Mean Reward: 304.8505989074707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Mean Reward: 214.39270935058593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Mean Reward: 279.42830657958984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Mean Reward: 191.01748962402343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Mean Reward: 318.2202423095703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Mean Reward: 177.66642684936522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Mean Reward: 305.10694885253906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Mean Reward: 199.37093811035157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Mean Reward: 189.43438796997071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Mean Reward: 300.1001663208008\n",
      "Epoch 60 test:\n",
      "Test Episode 1 Reward: 84.17875671386719\n",
      "Test Episode 2 Reward: 84.02227783203125\n",
      "Test Episode 3 Reward: 84.02159118652344\n",
      "Test Episode 4 Reward: 84.17875671386719\n",
      "Test Episode 5 Reward: 84.04368591308594\n",
      "Test Episode 6 Reward: 84.06700134277344\n",
      "Test Episode 7 Reward: 84.17875671386719\n",
      "Test Episode 8 Reward: 84.17875671386719\n",
      "Test Episode 9 Reward: 84.17875671386719\n",
      "Test Episode 10 Reward: 84.02261352539062\n",
      "Epoch 60 Model saved to ./checkpoints/deadly_corridor.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Mean Reward: 246.79349365234376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Mean Reward: 350.71970138549807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Mean Reward: 168.43794479370118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Mean Reward: 302.8137855529785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Mean Reward: 181.43633728027345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Mean Reward: 293.8214813232422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Mean Reward: 148.88097076416017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Mean Reward: 315.20680770874026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Mean Reward: 201.23438568115233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 22.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Mean Reward: 333.4384735107422\n",
      "Epoch 70 test:\n",
      "Test Episode 1 Reward: 294.95274353027344\n",
      "Test Episode 2 Reward: 294.95274353027344\n",
      "Test Episode 3 Reward: 294.95274353027344\n",
      "Test Episode 4 Reward: 294.95274353027344\n",
      "Test Episode 5 Reward: 337.1773681640625\n",
      "Test Episode 6 Reward: 563.5897827148438\n",
      "Test Episode 7 Reward: 312.7727966308594\n",
      "Test Episode 8 Reward: 294.95274353027344\n",
      "Test Episode 9 Reward: 757.5793914794922\n",
      "Test Episode 10 Reward: 279.5070037841797\n",
      "Epoch 70 Model saved to ./checkpoints/deadly_corridor.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Mean Reward: 247.86219177246093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Mean Reward: 387.7639678955078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Mean Reward: 203.09404449462892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 22.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Mean Reward: 378.9828620910645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Mean Reward: 199.167391204834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 25.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Mean Reward: 356.6153953552246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Mean Reward: 152.5507843017578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 23.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Mean Reward: 299.77059707641604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Mean Reward: 263.74560165405273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 21.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Mean Reward: 332.44045028686526\n",
      "Epoch 80 test:\n",
      "Test Episode 1 Reward: 294.95274353027344\n",
      "Test Episode 2 Reward: 294.95274353027344\n",
      "Test Episode 3 Reward: 550.5181121826172\n",
      "Test Episode 4 Reward: 259.6361541748047\n",
      "Test Episode 5 Reward: 294.95274353027344\n",
      "Test Episode 6 Reward: 567.1110076904297\n",
      "Test Episode 7 Reward: 294.95274353027344\n",
      "Test Episode 8 Reward: 281.06443786621094\n",
      "Test Episode 9 Reward: 294.95274353027344\n",
      "Test Episode 10 Reward: 294.95274353027344\n",
      "Epoch 80 Model saved to ./checkpoints/deadly_corridor.ckpt\n",
      "12060 time steps experienced during training\n"
     ]
    }
   ],
   "source": [
    "#For each time step, collect the following data:\n",
    "#The current game state\n",
    "#The action that was taken taken\n",
    "#The reward obtained from the chosen action\n",
    "#The next game state (store the first game state if the previous action ends the episode)\n",
    "#A variable indicating whether the episode is over yet\n",
    "\n",
    "tf.reset_default_graph()\n",
    "DQN = Q_network(learning_rate=learning_rate,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                channels=channels)\n",
    "exp_buffer = Buffer(size=buffer_size)\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if load_model == True:\n",
    "    print('Loading model from', model_dir)\n",
    "    tf.train.Saver().restore(session, model_dir)\n",
    "    \n",
    "elif load_model == False:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "game.set_sound_enabled(False)\n",
    "game.init()\n",
    "t = 0\n",
    "\n",
    "#Accumulate experiences in the buffer using an epsilon-greedy strategy with three training phases\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_rewards = list()\n",
    "    \n",
    "    for step in trange(steps_per_epoch, leave=True):\n",
    "        experience = list()\n",
    "        game.new_episode()\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            state1 = preprocess(np.concatenate((state.screen_buffer,\n",
    "                                                np.expand_dims(state.depth_buffer, axis=2)),\n",
    "                                                axis=2),\n",
    "                                               down_sample_ratio)\n",
    "            \n",
    "#Explore the environment by choosing random actions with 100% probability for the first phase of training\n",
    "\n",
    "            if epoch < 0.3*epochs:\n",
    "                action = np.random.randint(num_actions)\n",
    "            \n",
    "#Increase the probability of greedily choosing an action by a constant amount at each epoch in the second phase\n",
    "            \n",
    "            elif epoch < 0.9*epochs:\n",
    "                epsilon = start_epsilon - (epoch + 1 - 0.2*epochs)*(start_epsilon-end_epsilon)/(0.7*epochs)\n",
    "            \n",
    "                if np.random.uniform(0, 1) <= epsilon:\n",
    "                    action = np.random.randint(num_actions)\n",
    "                \n",
    "                else:\n",
    "                    action = DQN.choose_action(session, state1)[0]\n",
    "\n",
    "#Select a random action with 10% probability in the final phase of training\n",
    "                \n",
    "            else:\n",
    "                if np.random.uniform(0, 1) <= end_epsilon:\n",
    "                    action = np.random.randint(num_actions)\n",
    "                    \n",
    "                else:\n",
    "                    action = DQN.choose_action(session, state1)[0]\n",
    "\n",
    "            reward = game.make_action(actions[action], frame_delay)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done == False:\n",
    "                state = game.get_state()\n",
    "                state2 = preprocess(np.concatenate((state.screen_buffer,\n",
    "                                                    np.expand_dims(state.depth_buffer, axis=2)),\n",
    "                                                    axis=2),\n",
    "                                                    down_sample_ratio)\n",
    "        \n",
    "            elif done == True:\n",
    "                state2 = state1\n",
    "        \n",
    "#Add the experience obtained from each time step to the buffer\n",
    "\n",
    "            t += 1\n",
    "            exp_buffer.add_experience((state1, action, reward, state2, done))\n",
    "        \n",
    "#Sample a minibatch from the buffer if there are enough experiences in the buffer\n",
    "\n",
    "        if exp_buffer.length > batch_size:\n",
    "            s1, a, r, s2, terminal = exp_buffer.sample_buffer(batch_size)\n",
    "            \n",
    "#Train the Q-network by using the minibatch to update the action-value function Q\n",
    "            \n",
    "            Q2 = np.max(DQN.get_Q_values(session, s2), axis=1)\n",
    "            target_Q = DQN.get_Q_values(session, s1)\n",
    "            target_Q[np.arange(batch_size), a] = r + discount_factor*(1 - terminal)*Q2\n",
    "            DQN.calculate_loss(session, s1, target_Q)\n",
    "            \n",
    "        epoch_rewards.append(game.get_total_reward())\n",
    "        \n",
    "    print('Epoch {} Mean Reward: {}'.format(epoch + 1, np.mean(epoch_rewards)))\n",
    "    \n",
    "#Test the agent's performance for 10 episodes and save the model every 10 epochs\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 and epoch > 0:\n",
    "        print('Epoch {} test:'.format(epoch + 1))\n",
    "        test_agent(DQN, num_episodes=10, training=True,\n",
    "                   load_model=False, session=session, model_dir=model_dir)\n",
    "        \n",
    "        if save_model == True:\n",
    "            print('Epoch {} Model saved to {}'.format(epoch + 1, model_dir))\n",
    "            saver.save(session, model_dir, global_step=epoch + 1)\n",
    "        \n",
    "print('{} time steps experienced during training'.format(t))\n",
    "game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T16:28:46.194923Z",
     "start_time": "2018-01-19T16:28:45.724950Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./checkpoints/deadly_corridor.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/deadly_corridor.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/deadly_corridor.ckpt\n\t [[Node: save_1/RestoreV2_13 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_13/tensor_names, save_1/RestoreV2_13/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2_5/_51 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_106_save_1/RestoreV2_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'save_1/RestoreV2_13', defined at:\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\spyder\\utils\\ipython\\start_kernel.py\", line 268, in <module>\n    main()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\spyder\\utils\\ipython\\start_kernel.py\", line 264, in main\n    kernel.start()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-0ad9011c8a3c>\", line 3, in <module>\n    test_agent(DQN, num_episodes=20, training=False, load_model=True, model_dir=model_dir)\n  File \"<ipython-input-3-5b4e6bd2e895>\", line 47, in test_agent\n    tf.train.Saver().restore(sess, model_dir)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1218, in __init__\n    self.build()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1020, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/deadly_corridor.ckpt\n\t [[Node: save_1/RestoreV2_13 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_13/tensor_names, save_1/RestoreV2_13/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2_5/_51 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_106_save_1/RestoreV2_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"<ipython-input-6-0ad9011c8a3c>\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    test_agent(DQN, num_episodes=20, training=False, load_model=True, model_dir=model_dir)\n",
      "  File \u001b[0;32m\"<ipython-input-3-5b4e6bd2e895>\"\u001b[0m, line \u001b[0;32m47\u001b[0m, in \u001b[0;35mtest_agent\u001b[0m\n    tf.train.Saver().restore(sess, model_dir)\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\"\u001b[0m, line \u001b[0;32m1666\u001b[0m, in \u001b[0;35mrestore\u001b[0m\n    {self.saver_def.filename_tensor_name: save_path})\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\client\\session.py\"\u001b[0m, line \u001b[0;32m889\u001b[0m, in \u001b[0;35mrun\u001b[0m\n    run_metadata_ptr)\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\client\\session.py\"\u001b[0m, line \u001b[0;32m1120\u001b[0m, in \u001b[0;35m_run\u001b[0m\n    feed_dict_tensor, options, run_metadata)\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\client\\session.py\"\u001b[0m, line \u001b[0;32m1317\u001b[0m, in \u001b[0;35m_do_run\u001b[0m\n    options, run_metadata)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\client\\session.py\"\u001b[1;36m, line \u001b[1;32m1336\u001b[1;36m, in \u001b[1;35m_do_call\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m\u001b[1;31m:\u001b[0m Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/deadly_corridor.ckpt\n\t [[Node: save_1/RestoreV2_13 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_13/tensor_names, save_1/RestoreV2_13/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2_5/_51 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_106_save_1/RestoreV2_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'save_1/RestoreV2_13', defined at:\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\spyder\\utils\\ipython\\start_kernel.py\", line 268, in <module>\n    main()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\spyder\\utils\\ipython\\start_kernel.py\", line 264, in main\n    kernel.start()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-0ad9011c8a3c>\", line 3, in <module>\n    test_agent(DQN, num_episodes=20, training=False, load_model=True, model_dir=model_dir)\n  File \"<ipython-input-3-5b4e6bd2e895>\", line 47, in test_agent\n    tf.train.Saver().restore(sess, model_dir)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1218, in __init__\n    self.build()\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1020, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/deadly_corridor.ckpt\n\t [[Node: save_1/RestoreV2_13 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_13/tensor_names, save_1/RestoreV2_13/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2_5/_51 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_106_save_1/RestoreV2_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\n"
     ]
    }
   ],
   "source": [
    "#Test the fully trained model by only choosing actions with a greedy strategy\n",
    "\n",
    "test_agent(DQN, num_episodes=20, training=False, load_model=True, model_dir=model_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
